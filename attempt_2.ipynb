{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692de8cd-3a18-406c-a249-de7a2bfed7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a103185b-ac0d-4e8a-afbe-d9d752276f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954729b2-ab27-487a-80a8-754dfea3f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    TrainingArguments, Trainer, BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b7a1fc-d798-4f43-978a-5d1efae5f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # ok even if training args use fp16\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # same model you specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ef7c06-cf58-4801-b64e-de214daf5460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2078d7f4b0704711b4ac4eae7d99489a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Model & tokenizer ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# IMPORTANT: For LLaMA-2, use EOS as pad to avoid adding new tokens (and device-side asserts).\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# Make the model aware of the pad id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e11c3b6-0669-4107-891a-98a60fde6991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "# ---------- LoRA ----------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # slightly larger rank is typical; keep small if VRAM tight\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # same as your code\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Prepare for k-bit training (handles layer norms, gradients, etc.)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f6cc86-c72c-403f-90ae-353718ba5cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419e29e5eefc49ac86e27e76328c0f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Data ----------\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Keep IMDB (works fine for LM). Optionally, switch to wikitext by uncommenting the next two lines:\n",
    "# dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:5%]\")\n",
    "# text_field = \"text\"\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:5000]\")  # smaller slice to fit 7B+LoRA memory\n",
    "text_field = \"text\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    out = tokenizer(\n",
    "        examples[text_field],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",  # consistent shape for fp16/4-bit training\n",
    "    )\n",
    "    # For causal LM, labels are the input_ids (shift is done inside model)\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c05d1b2-5ba1-4b16-b3ff-aa0d4f9fff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 18:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.072800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.023100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=2.184054377741707, metrics={'train_runtime': 1110.7351, 'train_samples_per_second': 4.502, 'train_steps_per_second': 0.282, 'total_flos': 5.07766112256e+16, 'train_loss': 2.184054377741707, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Training ----------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,     # lower batch for 7B + 4-bit + LoRA on common GPUs\n",
    "    gradient_accumulation_steps=8,     # effective batch = 16\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    num_train_epochs=1,                # start small; increase once stable\n",
    "    fp16=True,                         # mixed precision; ok with 4-bit quant\n",
    "    bf16=False,                        # keep False unless your GPU supports bf16 well\n",
    "    learning_rate=2e-4,                # typical LoRA LR\n",
    "    weight_decay=0.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ea2ee6-773e-40b7-b83b-1c343e499c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete and model saved at ./llama2-imdb-lora\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save locally ----------\n",
    "model.save_pretrained(\"./llama2-imdb-lora\")\n",
    "tokenizer.save_pretrained(\"./llama2-imdb-lora\")\n",
    "\n",
    "print(\"✅ Training complete and model saved at ./llama2-imdb-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d5a88-5a94-4e1a-86d1-b8b8fb33e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model for inference\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"./llama2-imdb-lora\")\n",
    "ft_model.eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"The movie was absolutely wonderful because\"\n",
    "outputs = pipe(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf65704-78ac-4bc5-a3be-1f88ee4cc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side by side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef746c3-b083-49ee-81e4-10222ff91189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773c4d62da034d80925274487908f663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Prompt: The movie was absolutely wonderful because\n",
      "\n",
      "--- Base Model Output ---\n",
      "The movie was absolutely wonderful because of the people in it. I have never seen such a talented group of actors, with such a good story, in a movie so bad. The plot was so bad that it was good. The acting was so good that it was bad. The acting was so good that it was good. The acting was so bad that it was good. I can't believe they made this movie.\n",
      "\n",
      "--- Fine-tuned Model Output ---\n",
      "The movie was absolutely wonderful because of the cast. I loved the chemistry between the leads, the supporting cast was perfect, and the story was really interesting and funny. However, the direction was terrible. The director should have cut out the horrible exposition at the beginning and the end and just focused on the relationship between the two leads. The story was told very well and it was so funny, but the ex\n",
      "================================================================================\n",
      "Prompt: The film was terrible and I would not recommend it because\n",
      "\n",
      "--- Base Model Output ---\n",
      "The film was terrible and I would not recommend it because it was just plain bad. It was a really bad film with a really bad storyline and the acting was atrocious. I mean come on, I've seen bad acting in films before, but this was just plain terrible. The film was very slow and boring and I was like \"oh my god, how much longer do I have to sit through this\". I mean I was\n",
      "\n",
      "--- Fine-tuned Model Output ---\n",
      "The film was terrible and I would not recommend it because of the following: 1. It was very very predictable. 2. It was very very poorly written. 3. It was very very poorly directed. 4. It was very very poorly acted. 5. It was very very boring. 6. It was very very unoriginal. 7. It was very very cheesy. 8\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# ----------------- Load tokenizer -----------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ----------------- Load base model -----------------\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "pipe_base = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# ----------------- Load fine-tuned model -----------------\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"./llama2-imdb-lora\")\n",
    "ft_model.eval()\n",
    "\n",
    "pipe_ft = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# ----------------- Test prompts -----------------\n",
    "prompts = [\n",
    "    \"The movie was absolutely wonderful because\",\n",
    "    \"The film was terrible and I would not recommend it because\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"\\n--- Base Model Output ---\")\n",
    "    out_base = pipe_base(prompt, max_new_tokens=80, do_sample=True, temperature=0.7)\n",
    "    print(out_base[0][\"generated_text\"])\n",
    "\n",
    "    print(\"\\n--- Fine-tuned Model Output ---\")\n",
    "    out_ft = pipe_ft(prompt, max_new_tokens=80, do_sample=True, temperature=0.7)\n",
    "    print(out_ft[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f10f45-afd2-4195-bc12-e284a4ec168f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb8e2a-8300-4021-99cc-94e1fc28a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82e0ad-178d-4422-83c1-2d24ab36455b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
