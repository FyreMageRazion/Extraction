{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05187286-cfcd-4664-a50d-95e665ab11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U torch transformers datasets bitsandbytes accelerate peft[torch] evaluate sentencepiece\n",
    "# pip install huggingface_hub[hf_xet]\n",
    "# pip install hf_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fd016-90ba-4977-86e2-2dd7cec25123",
   "metadata": {},
   "source": [
    "# BERT QLoRA + LoRA fine-tuning on IMDB sentiment dataset \n",
    "(dipanjanS/imdb_sentiment_finetune_dataset20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b44d462-3bb5-493c-9053-725a22913211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, get_peft_model_state_dict\n",
    "import evaluate\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941fc68-ad5c-49c0-848c-da3e4163ffb1",
   "metadata": {},
   "source": [
    "This experiment was performed on the following Runpod instance:\n",
    "<br>1x A40 (48 GB VRAM)\n",
    "<br>50 GB RAM â€¢ 9 vCPU\n",
    "<br>Total Disk: 80 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22880af0-611c-4753-a37b-1cd06df6d71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device count:', torch.cuda.device_count())\n",
    "    print('Current device:', torch.cuda.current_device())\n",
    "    print('Device name:', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c576d0c2-102d-46e6-8c9f-231582642362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATASET = \"dipanjanS/imdb_sentiment_finetune_dataset20k\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2 # First iteration was run with 3 epocs and then repeated with 2 epocs for the current final outputs in this notebook\n",
    "LEARNING_RATE = 2e-4\n",
    "OUTPUT_DIR = \"./bert_qlora_imdb_output\"\n",
    "SEED = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "torch.manual_seed(SEED) # Congiguring this for reproducibility\n",
    "raw_ds = load_dataset(DATASET)\n",
    "print(raw_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c28491-737c-45b3-a9fb-2b449642ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 20000\n"
     ]
    }
   ],
   "source": [
    "if 'train' in raw_ds and len(raw_ds) == 1:\n",
    "    ds = raw_ds['train']\n",
    "else:\n",
    "    from datasets import concatenate_datasets\n",
    "    allsplits = [raw_ds[s] for s in raw_ds]\n",
    "    ds = concatenate_datasets(allsplits)\n",
    "\n",
    "\n",
    "print('Total examples:', len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bcc660-9247-4a10-a3eb-26386f96cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 16000 Test size: 4000\n"
     ]
    }
   ],
   "source": [
    "# Going for the 80/20 train/test split\n",
    "split = ds.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds = split['train']\n",
    "test_ds = split['test']\n",
    "print('Train size:', len(train_ds), 'Test size:', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76309cea-1343-4ca1-926b-d0eed9961b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9967fbfef59459792d199b7fb19c198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(1), 'input_ids': tensor([  101,  2157,  1996,  2902,  3496,  2007,  9079,  1998, 10881,  2001,\n",
      "         2589,  8235,  2135,  2009,  4627,  2125,  2007, 11939,  2006,  1037,\n",
      "        25061,  2559,  2200,  6649,  5229,  1010,  1996,  9986,  2015,  2679,\n",
      "         2014,  2046,  1037, 24501,  2271, 26243,  3370,  2282,  1004,  2027,\n",
      "         2693,  2014,  2013,  1996, 25061,  3031,  1037,  2793,  1998, 10975,\n",
      "         5657,  4324,  2014,  2192,  2013,  2008,  2391,  2006,  2009,  2003,\n",
      "         5793,  2008, 11939,  2003,  2383,  1037,  2843,  1997,  4390,  5505,\n",
      "         1998,  2014,  8948,  2024,  7989,  1010,  2004,  2016,  4332,  2000,\n",
      "        11693,  1997, 10975,  5657,  2000,  2025,  2681,  2014,  2217,  2016,\n",
      "        16680,  1000,  2123,  1005,  1056,  2175,  1045,  2293,  2017,  1998,\n",
      "         2059,  2014,  8187,  9010,  1998,  2016,  3632,  2046, 15050,  6545,\n",
      "         1004,  1996,  8080,  3065,  1037,  3154,  4257,  2240,  1004,  1996,\n",
      "        11500,  2175,  2046,  2440,  2041, 12603,  5549,  1004,  3288,  1999,\n",
      "         1037, 13366, 12322, 24714,  8844, 10975,  5657,  4084,  2067,  2013,\n",
      "         1996,  2793,  1999,  5469,  2004,  1996,  7435,  9652,  3046,  2000,\n",
      "         5213,  2014,  5996,  5208,  2540,  2021,  2045,  2003,  2053,  3433,\n",
      "         1998,  2016,  2003, 13800,  3973,  8793,  2757,  2092,  2307,  3496,\n",
      "         2092,  2589,  3057,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "    examples['review'],\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "# Just incase pre-procressing\n",
    "train_tok = train_ds.map(preprocess_function, batched=True, remove_columns=[c for c in train_ds.column_names if c != 'sentiment' and c != 'review'])\n",
    "test_tok = test_ds.map(preprocess_function, batched=True, remove_columns=[c for c in test_ds.column_names if c != 'sentiment' and c != 'review'])\n",
    "train_tok = train_tok.rename_column('sentiment', 'labels')\n",
    "test_tok = test_tok.rename_column('sentiment', 'labels')\n",
    "\n",
    "# Set format to PyTorch\n",
    "train_tok.set_format(type='torch', columns=[c for c in train_tok.column_names if c != 'review'])\n",
    "test_tok.set_format(type='torch', columns=[c for c in test_tok.column_names if c != 'review'])\n",
    "\n",
    "print(train_tok[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3c056-47c1-4e3c-bb6e-f4fd2e589859",
   "metadata": {},
   "source": [
    "# Applying LORA and QLORA Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4387f769-3f57-4ff9-b366-8534b68a8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Since this is an experiment, performing 4 bit quantization for lowest cost\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf8b3d6b-13a6-43d9-a43c-8db272834140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model in 4-bit mode (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loadint the model in 4-bit mode\n",
    "print('\\nLoading model in 4-bit mode (this may take a while)...')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "# resize token embeddings if tokenizer changed\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca992871-1d30-46bd-926e-5c8e08780529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training (patching some layers for stability)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config & applying PEFT\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # BERT attention modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a90e0a-bd28-491a-983c-b090c125024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c48b9c-730e-4555-a817-a4419ccb6441",
   "metadata": {},
   "source": [
    "## Comparing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb2914d-68de-4ca1-90a4-d6cc5b336272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 67607812\n",
      "Trainable params (after LoRA): 886274\n",
      "Trainable fraction: 0.013109\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print('Total params:', total_params)\n",
    "print('Trainable params (after LoRA):', trainable_params)\n",
    "print('Trainable fraction: {:.6f}'.format(trainable_params/total_params))\n",
    "\n",
    "\n",
    "# Saving incase instance shut down\n",
    "with open(os.path.join(OUTPUT_DIR, 'param_counts.txt'), 'w') as f:\n",
    "    f.write(f\"total={total_params}\\ntrainable={trainable_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c556d6-b1b3-4c86-848b-1b39ce8b83cc",
   "metadata": {},
   "source": [
    "## Setting up evaluation and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5816a4b8-16f5-48d8-a367-6faa4ace6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ced0cf971364e5a837690fbc84033a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d703be6d614be9bd1bfd748cba03c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d312b136d84307b6f3d09c3451bc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy.compute(predictions=preds, references=labels)\n",
    "    p = precision.compute(predictions=preds, references=labels, average='binary')\n",
    "    r = recall.compute(predictions=preds, references=labels, average='binary')\n",
    "    f_1 = f1.compute(predictions=preds, references=labels, average='binary')\n",
    "    return {'accuracy': acc['accuracy'], 'precision': p['precision'], 'recall': r['recall'], 'f1': f_1['f1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d841f535-ac4e-44f9-8573-410aa0148670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1214/2676419255.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    report_to=[\"none\"],\n",
    "    gradient_accumulation_steps=1,\n",
    "    seed=SEED,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=test_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c43673b-dcfb-4da3-9756-42de4c92ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated before training (bytes): 144471552\n"
     ]
    }
   ],
   "source": [
    "# This code execution will not involve weights and biases logging - however I understand how API KEY can be obtained and configurations and be added to log \n",
    "# import wandb\n",
    "# wandb.init(project='bert-qlora-imdb', name='bert-qlora-run')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    before_mem = torch.cuda.memory_allocated()\n",
    "    print('GPU memory allocated before training (bytes):', before_mem)\n",
    "else:\n",
    "    print('No CUDA device; memory measurements will be skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2cebb8f-89bd-4fea-bb8d-e2403e9a1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49cb6dc-e1a5-4b4f-9a2d-27682e70eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 04:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.146100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (s): 278.9815013408661\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "print('Training time (s):', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a81d8b-c8cc-487f-921b-f60702cf2d84",
   "metadata": {},
   "source": [
    "## Delta memory - memory allocated before training - memory allocated after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afae67d3-8778-43d8-a2a0-4a9e6d0cde24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated after training (bytes): 175854080\n",
      "Delta (bytes): 31382528\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    after_mem = torch.cuda.memory_allocated()\n",
    "    print('GPU memory allocated after training (bytes):', after_mem)\n",
    "    print('Delta (bytes):', after_mem - before_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea84e5-a8dd-4898-8a7b-80473f360f92",
   "metadata": {},
   "source": [
    "## Saving the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caacb081-09d2-4a56-a33b-76e65a2d01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(OUTPUT_DIR, 'qlora_lora_model'))\n",
    "# Save the PEFT adapter separately\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, 'qlora_lora_adapter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8961b-d617-4fe1-a4c4-07fdf3ab27e0",
   "metadata": {},
   "source": [
    "## Executing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c60c1dc-58a8-4097-ae00-b27a2dfee385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.3117099702358246, 'eval_accuracy': 0.909, 'eval_precision': 0.8947368421052632, 'eval_recall': 0.9238191975622143, 'eval_f1': 0.9090454772613693, 'eval_runtime': 9.5187, 'eval_samples_per_second': 420.224, 'eval_steps_per_second': 26.264, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=test_tok)\n",
    "print('Eval metrics:', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a332f88e-5a6c-4dd2-88d1-edc4e784f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current total params: 67607812\n",
      "Current trainable params (LoRA adapters): 886274\n"
     ]
    }
   ],
   "source": [
    "print('Current total params:', total_params)\n",
    "print('Current trainable params (LoRA adapters):', trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495e2a5-4022-45e4-a50b-c8e804399e2f",
   "metadata": {},
   "source": [
    "### Additonal comments\n",
    "Logging in weights and biases requires additional API Keys and configuration.\n",
    "To compare benefits of QLora to full finetuning - I would have to run full fine tuning in RunPod for same number of EPOCs and compare memory allocation, trainable parameters and performance evaluation on same test data. I understand the process, however, it is not executed in this notebook to conserve costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc5150-9cac-4b43-9f2d-95189f0b002b",
   "metadata": {},
   "source": [
    "## Overall summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dff90d1c-f7ae-4fa1-a727-1da7da975cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16,\n",
      " 'dataset': 'dipanjanS/imdb_sentiment_finetune_dataset20k',\n",
      " 'epochs': 2,\n",
      " 'eval_metrics': {'epoch': 2.0,\n",
      "                  'eval_accuracy': 0.909,\n",
      "                  'eval_f1': 0.9090454772613693,\n",
      "                  'eval_loss': 0.3117099702358246,\n",
      "                  'eval_precision': 0.8947368421052632,\n",
      "                  'eval_recall': 0.9238191975622143,\n",
      "                  'eval_runtime': 9.5187,\n",
      "                  'eval_samples_per_second': 420.224,\n",
      "                  'eval_steps_per_second': 26.264},\n",
      " 'learning_rate': 0.0002,\n",
      " 'max_length': 256,\n",
      " 'model_name': 'bert-base-uncased',\n",
      " 'total_params': 67607812,\n",
      " 'trainable_params': 886274}\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "'model_name': MODEL_NAME,\n",
    "'dataset': DATASET,\n",
    "'max_length': MAX_LENGTH,\n",
    "'batch_size': BATCH_SIZE,\n",
    "'epochs': EPOCHS,\n",
    "'learning_rate': LEARNING_RATE,\n",
    "'total_params': total_params,\n",
    "'trainable_params': trainable_params,\n",
    "'eval_metrics': metrics,\n",
    "}\n",
    "\n",
    "\n",
    "pprint(summary)\n",
    "\n",
    "# Saving incase instance shut down\n",
    "with open(os.path.join(OUTPUT_DIR, 'summary.txt'), 'w') as f:\n",
    "    f.write(str(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b96bf9-6c1f-4a91-b984-040d602b3508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59356d-d3ab-4a0b-9487-986ba5821b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cfee2-d289-4e9b-a8cb-3557bc2a32d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
