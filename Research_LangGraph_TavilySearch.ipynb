{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd298ac-8750-449c-b097-4055e8640939",
   "metadata": {},
   "source": [
    "## LangGraph with Tavily - Serves as Code + Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6951803-2449-4534-a3f9-e3ce9a1f4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from typing import TypedDict, Annotated, List, Literal, Optional, Any, Dict, Annotated\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "from google.genai import types\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState,START, END\n",
    "\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d2a8d-4125-40c9-ab30-d2d11d9f6144",
   "metadata": {},
   "source": [
    "## Configuring API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "521a3fb2-0783-44b4-aa98-3076b41016a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = \"<API_KEY>\"\n",
    "tavily_api_key = \"<API_KEY>\"\n",
    "    \n",
    "os.environ[\"GEMINI_API_KEY\"] = google_api_key\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "genai.configure(api_key=google_api_key)\n",
    "tavily_client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4c781-2433-4044-bdec-743b07d423c5",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b2a9d85-fa5c-428e-86d9-c28d081a4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_questions_system_prompt = \"\"\"ROLE:\n",
    "You are an analytical assistant specialized in expanding vague user queries into multiple web-searchable, insight-rich research questions focused on Large Language Models (LLMs), fine-tuning, and AI model optimization.\n",
    "\n",
    "STRICT INSTRUCTION:\n",
    "STRICTLY RESPOND ONLY WITH THE FINAL LIST OF QUESTIONS IN THE GIVEN FORMAT.\n",
    "\n",
    "MAIN OBJECTIVE:\n",
    "Transform the given user query into a structured list of 5 distinct and meaningful questions that explore the topic from diverse research and technical perspectives.\n",
    "\n",
    "<INPUT_USER_QUERY>\n",
    "{user_query}\n",
    "<INPUT_USER_QUERY_END>\n",
    "\n",
    "CONTEXT:\n",
    "The analysis supports research and innovation in LLM development, with the following stakeholder focuses:\n",
    "ML Researchers & Engineers – Interested in architectures, training methods, and fine-tuning efficiency.\n",
    "Data Scientists & Practitioners – Focused on dataset design, evaluation metrics, and real-world deployment.\n",
    "Enterprise AI Teams – Concerned with model customization, cost optimization, and scalability.\n",
    "Ethics & Compliance Experts – Focused on safety, bias mitigation, and responsible fine-tuning.\n",
    "Investors & Policy Makers – Interested in market trends, research funding, and open-source vs proprietary innovation.\n",
    "\n",
    "GUIDELINES:\n",
    "\n",
    "Generate exactly 5 self-contained, web-search-ready questions.\n",
    "Each question must explore the topic from a unique dimension, such as:\n",
    "Model architecture and training efficiency\n",
    "Fine-tuning techniques and data quality\n",
    "Evaluation metrics and benchmarks\n",
    "Ethical, safety, or regulatory implications\n",
    "Commercial and research trends in LLM innovation\n",
    "Avoid vague or repetitive phrasing.\n",
    "Ensure each question can stand alone for online search or academic exploration.\n",
    "EXPECTED OUTPUT FORMAT (strictly adhere to this schema):\n",
    "\n",
    "{{\n",
    "  \"listOfQuestions\": [\n",
    "    {{\"question\": \"<Question 1>\"}},\n",
    "    {{\"question\": \"<Question 2>\"}},\n",
    "    {{\"question\": \"<Question 3>\"}},\n",
    "    {{\"question\": \"<Question 4>\"}},\n",
    "    {{\"question\": \"<Question 5>\"}}\n",
    "  ]\n",
    "}}\n",
    "This output must be a valid JSON object compatible with the following Pydantic schema:\n",
    "class question(BaseModel):\n",
    "    question: str = Field(..., description=\"Individual questions from the output\")\n",
    "\n",
    "class listOfQuestions(BaseModel):\n",
    "    listOfQuestions: list[question] = Field(..., description=\"List of various questions\")\n",
    "EXAMPLE INPUT:\n",
    "USER QUERY: \"How can LLMs be fine-tuned efficiently?\"\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{{\n",
    "  \"listOfQuestions\": [\n",
    "    {{\"question\": \"What are the most cost-efficient fine-tuning strategies for large language models in 2025?\"}},\n",
    "    {{\"question\": \"How do data curation and sampling techniques impact LLM fine-tuning quality and generalization?\"}},\n",
    "    {{\"question\": \"What evaluation metrics are most reliable for assessing fine-tuned LLM performance across tasks?\"}},\n",
    "    {{\"question\": \"How can parameter-efficient fine-tuning (PEFT) methods reduce resource usage without compromising accuracy?\"}},\n",
    "    {{\"question\": \"What are the latest ethical and compliance considerations when fine-tuning LLMs on domain-specific data?\"}}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "summary_system_prompt = \"\"\"\n",
    "ROLE:\n",
    "You are an intelligent analytical assistant that produces concise, evidence-based executive reports. Your task is to read the provided web search results and context, reason across findings, and build on them to propose new ideas, methodologies, and an actionable implementation plan.\n",
    "\n",
    "STRICT INSTRUCTION:\n",
    "Respond only with the executive report described in the “EXPECTED REPORT STRUCTURE” section below. Do not include meta commentary, process logs, or extraneous headers.\n",
    "\n",
    "<User Query>\n",
    "{user_query}\n",
    "</User Query>\n",
    "\n",
    "Use the following web search results and context when producing the report.\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "GUIDELINES:\n",
    "\n",
    "Use all relevant sources from the context and web search results; integrate and reconcile conflicting evidence. Cite the top 4 most important sources inline (author or short title + date) and list full references in the Appendix.\n",
    "Apply explicit logical reasoning: show the chain of inference that connects evidence → insight → proposed methodology. Keep reasoning concise (no longer than 2–4 short sentences per inference).\n",
    "Focus on actionability: every proposed idea or methodology must include (a) rationale, (b) required inputs/resources, (c) first-pass experimental design or implementation steps, and (d) measurable success criteria.\n",
    "Highlight uncertainty: for each recommendation mark confidence level (High / Medium / Low) and key assumptions.\n",
    "Produce visuals where helpful (suggested table contents, sample KPI dashboard layout, or step-by-step checklist). If the assistant can generate charts/tables in the environment, produce them; otherwise describe clearly what the visual should contain.\n",
    "Keep the report executive-friendly: concise bullets, numbered lists, and short tables. Total length: aim for 1–2 pages (≈600–900 words). Longer appendices are acceptable.\n",
    "If the provided context is insufficient, explicitly state which high-value evidence is missing and propose 2–3 next research actions to obtain it (e.g., datasets, experiments, stakeholder interviews).\n",
    "\n",
    "EXPECTED REPORT STRUCTURE (strictly follow):\n",
    "\n",
    "Title & One-Line Purpose\n",
    "Title: <short, descriptive>\n",
    "\n",
    "Purpose: One sentence describing what the report answers.\n",
    "Executive Summary (3–5 bullets)\n",
    "Key insights (most important findings)\n",
    "Principal recommendation(s) and expected impact\n",
    "Key Evidence & Synthesis (bulleted list + 1 short synthesis paragraph)\n",
    "Top 4 evidence points (each 1 line, with inline citation)\n",
    "Short synthesis: how these pieces of evidence combine to form a higher-level insight.\n",
    "Logical Reasoning & Inference Chain\n",
    "3–6 numbered inference steps: Evidence → Logical step → Conclusion/implication (each 1–2 lines).\n",
    "Proposed Ideas & New Methodologies\n",
    "For each proposed idea/methodology (limit to 3 top proposals):\n",
    "\n",
    "Name (1 line)\n",
    "Rationale (1 line) with citation(s)\n",
    "High-level method / steps (3–6 numbered bullets)\n",
    "Required inputs / resources (data, compute, personnel)\n",
    "Success criteria / KPIs (quantitative where possible)\n",
    "Confidence level (High/Medium/Low) and key assumptions\n",
    "Implementation Roadmap (phased)\n",
    "\n",
    "Phase 0 — Quick wins (0–4 weeks): 2–4 actions\n",
    "Phase 1 — Pilot (1–3 months): deliverables and evaluation plan\n",
    "Phase 2 — Scale (3–12 months): operationalization steps and monitoring\n",
    "\n",
    "Risks & Mitigations\n",
    "\n",
    "Top 5 risks (short bullets) with one mitigation per risk and residual risk level.\n",
    "Cost & Resource Estimate (ballpark)\n",
    "Provide a short table or bullets estimating key cost drivers (data acquisition, compute, personnel) in high/medium/low buckets.\n",
    "KPIs & Dashboard (suggested)\n",
    "List 6–8 KPIs to monitor progress and impact; briefly explain why each matters. If relevant, suggest visualization types (trend, bar, heatmap).\n",
    "Recommendations (concise, prioritized list: Top 3)\n",
    "Each recommendation in one short sentence and the expected business/technical value.\n",
    "\n",
    "Appendix\n",
    "\n",
    "Full citations (top sources used)\n",
    "Additional details: experiment designs, data schema, or sample code snippets (optional).\n",
    "\n",
    "INTENDED BENEFICIARIES (adapt as needed):\n",
    "Senior Executives & Strategy Teams — quick decision support and prioritized recommendations.\n",
    "Product & Engineering Managers — concrete pilot designs and resource needs.\n",
    "Research & Data Science Teams — methodological proposals and evaluation plans.\n",
    "Compliance & Risk — highlighted risks and mitigations for review.\n",
    "Investors & Stakeholders — expected impact and high-level cost picture.\n",
    "\n",
    "QUALITY & FORMATTING RULES:\n",
    "\n",
    "Use plain, professional language.\n",
    "Bulleted and numbered lists preferred over long paragraphs.\n",
    "Provide inline citations for claims that derive from specific sources.\n",
    "Keep the main report to 1–2 pages; place deeper technical detail in the Appendix.\n",
    "If web search results disagree, summarize both viewpoints and state which you weight more and why.\n",
    "\"\"\"\n",
    "\n",
    "class question(BaseModel):\n",
    "    question: str = Field(..., description=\"Individual questions from the output\")\n",
    "\n",
    "class listOfQuestions(BaseModel):\n",
    "    listOfQuestions: list[question] = Field(..., description=\"List of various questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a72d6-40fa-41b8-b086-adea16c01027",
   "metadata": {},
   "source": [
    "## Declaring LangGraph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d73bdb4-6e31-41e7-a7bc-2fafbce9dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAnalysisState(BaseModel):\n",
    "    user_question: Annotated[str, operator.add] = None\n",
    "    search_query:Annotated[str, operator.add] = None\n",
    "    list_of_search_questions: Annotated[list, operator.add]  = []\n",
    "    list_of_search_responses: Annotated[list, operator.add]  = []\n",
    "    log_messages: Annotated[list, operator.add]  = []\n",
    "    final_summary: Annotated[str, operator.add] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410a0ed-38bf-480d-a0c5-7dff85558ecb",
   "metadata": {},
   "source": [
    "## Initial refinement of the user query into broader questions to be searched online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "435ee4eb-ea93-4055-963f-404dafed0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get search questions\n",
    "def get_search_questions(state: ResearchAnalysisState):\n",
    "    dynamic_data = {\"user_query\":state.user_question}\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = model.generate_content(\n",
    "        search_questions_system_prompt.format(**dynamic_data)\n",
    "    )\n",
    "    \n",
    "    text = response.candidates[0].content.parts[0].text\n",
    "    print(text)\n",
    "    response2 = model.generate_content(\n",
    "        \"Extract the list of 5 questions in a form of a json list from the following text \\n\\n\" + text,\n",
    "    generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=listOfQuestions,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    text2 = response2.candidates[0].content.parts[0].text\n",
    "    print(text2)\n",
    "    data = json.loads(text2)\n",
    "    questions = [q[\"question\"] for q in data[\"listOfQuestions\"]]\n",
    "    state.list_of_search_questions = questions\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f1b6d-b0cd-4cdb-9007-7fcd9d1424f4",
   "metadata": {},
   "source": [
    "## Tavily Search Tool & Final Executive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fed42d8-db3f-4802-a76f-86fae3b06d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# online search needed?\n",
    "def should_continue(state: ResearchAnalysisState) -> bool:\n",
    "    if len(state.list_of_search_questions)>0:\n",
    "        return 'search_tool'\n",
    "    else:\n",
    "        return 'summary'\n",
    "        \n",
    "    \n",
    "def search_tool(state: ResearchAnalysisState):\n",
    "    all_results = []\n",
    "    \n",
    "    # Collect results for all queries\n",
    "    for query in state.list_of_search_questions:\n",
    "        print(f\"\\n--- Searching Tavily for: '{query}' ---\")\n",
    "        search_query = state.user_question + query\n",
    "        response = tavily_client.search(query=query, search_depth=\"advanced\", max_results=2)\n",
    "        results_summary = [\n",
    "            f\"Source: {r['url']}\\nSnippet: {r['content']}\" for r in response.get('results', [])\n",
    "        ]\n",
    "        if not results_summary:\n",
    "            results_summary = [\"No relevant search results found for the query.\"]\n",
    "        \n",
    "        all_results.append(results_summary)\n",
    "\n",
    "    state.list_of_search_responses.extend(all_results)\n",
    "    state.list_of_search_questions = []  \n",
    "    \n",
    "    return state\n",
    "\n",
    "def summary(state: ResearchAnalysisState):\n",
    "    print(f\"\\n--- Final Summary preparation on the way: '{state.user_question}' ---\")\n",
    "    dynamic_data = {\"user_query\":state.user_question,\n",
    "                   \"context\": '<NEW SEARCH RESULT>'.join([x[0] for x in state.list_of_search_responses]) }\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = model.generate_content(\n",
    "        summary_system_prompt.format(**dynamic_data)\n",
    "    )\n",
    "    state.final_summary = response.candidates[0].content.parts[0].text\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0b351-de2a-4892-9988-fcfbd245363d",
   "metadata": {},
   "source": [
    "## Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "011dda47-ac38-48cf-94d0-d30d0a3c7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ResearchAnalysisState)\n",
    "\n",
    "# Only add your regular nodes\n",
    "graph.add_node(\"get_search_questions\", get_search_questions)\n",
    "graph.add_node(\"search_tool\", search_tool)\n",
    "graph.add_node(\"summary\", summary)\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"get_search_questions\")\n",
    "graph.add_edge(\"get_search_questions\", \"search_tool\")\n",
    "graph.add_edge(\"search_tool\", \"summary\")\n",
    "graph.add_edge(\"summary\", END)\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b2157-7cac-464e-8eea-403925c4009d",
   "metadata": {},
   "source": [
    "## Visualizing the langraph orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1e93ef5-a0e4-4706-8d2c-dfd7f198be66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e61c9-ed1b-4537-a28d-c6477755ff9d",
   "metadata": {},
   "source": [
    "## Executing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca3954d1-94cb-43fe-abab-c5c2242130a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"listOfQuestions\": [\n",
      "    {\"question\": \"What novel neural network architectures facilitate low-compute fine-tuning while maintaining high LLM performance?\"},\n",
      "    {\"question\": \"How can active learning and reinforcement learning be integrated into LLM fine-tuning to minimize data requirements and maximize quality?\"},\n",
      "    {\"question\": \"What are the most effective knowledge distillation techniques for transferring knowledge from large LLMs to smaller, fine-tuned models with minimal performance loss?\"},\n",
      "    {\"question\": \"How do different quantization and pruning methods impact the trade-off between computational cost and accuracy during LLM fine-tuning?\"},\n",
      "    {\"question\": \"What benchmarks and evaluation protocols are best suited to assess the quality and efficiency of novel LLM fine-tuning methods across diverse tasks?\"}\n",
      "  ]\n",
      "}\n",
      "```\n",
      "{\n",
      "  \"listOfQuestions\": [\n",
      "    {\"question\": \"What novel neural network architectures facilitate low-compute fine-tuning while maintaining high LLM performance?\"},\n",
      "    {\"question\": \"How can active learning and reinforcement learning be integrated into LLM fine-tuning to minimize data requirements and maximize quality?\"},\n",
      "    {\"question\": \"What are the most effective knowledge distillation techniques for transferring knowledge from large LLMs to smaller, fine-tuned models with minimal performance loss?\"},\n",
      "    {\"question\": \"How do different quantization and pruning methods impact the trade-off between computational cost and accuracy during LLM fine-tuning?\"},\n",
      "    {\"question\": \"What benchmarks and evaluation protocols are best suited to assess the quality and efficiency of novel LLM fine-tuning methods across diverse tasks?\"}\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- Searching Tavily for: 'What novel neural network architectures facilitate low-compute fine-tuning while maintaining high LLM performance?' ---\n",
      "\n",
      "--- Searching Tavily for: 'How can active learning and reinforcement learning be integrated into LLM fine-tuning to minimize data requirements and maximize quality?' ---\n",
      "\n",
      "--- Searching Tavily for: 'What are the most effective knowledge distillation techniques for transferring knowledge from large LLMs to smaller, fine-tuned models with minimal performance loss?' ---\n",
      "\n",
      "--- Searching Tavily for: 'How do different quantization and pruning methods impact the trade-off between computational cost and accuracy during LLM fine-tuning?' ---\n",
      "\n",
      "--- Searching Tavily for: 'What benchmarks and evaluation protocols are best suited to assess the quality and efficiency of novel LLM fine-tuning methods across diverse tasks?' ---\n",
      "\n",
      "--- Final Summary preparation on the way: 'How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?' ---\n"
     ]
    }
   ],
   "source": [
    "initial_state = ResearchAnalysisState(\n",
    "    user_question=\"How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?\",\n",
    "    log_messages=[]\n",
    ")\n",
    "\n",
    "final_state = app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f0866-2370-424c-86a2-f392585177cc",
   "metadata": {},
   "source": [
    "## Final Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "900c4e15-4556-481b-b46e-6e21305f2865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Title: Efficient LLM Fine-tuning for Resource-Constrained Environments\n",
       "\n",
       "Purpose: This report proposes a novel, compute-efficient LLM fine-tuning methodology to achieve high model quality, focusing on techniques suitable for resource-constrained environments.\n",
       "\n",
       "Executive Summary\n",
       "\n",
       "*   Combining knowledge distillation (IBM, n.d.), quantization (deepchecks, n.d.), and active learning (Intuition Labs, n.d.) offers a path to efficient and effective LLM fine-tuning.\n",
       "*   Knowledge distillation enables transferring knowledge from large models to smaller ones, reducing computational demands.\n",
       "*   Quantization reduces model size and improves computing efficiency with minimal performance degradation (deepchecks, n.d.).\n",
       "*   Active learning optimizes data usage by strategically selecting the most informative samples for human labeling and fine-tuning (Intuition Labs, n.d.).\n",
       "*   We recommend a phased implementation, starting with knowledge distillation, followed by quantization and active learning integration, to incrementally improve efficiency and quality.\n",
       "\n",
       "Key Evidence & Synthesis\n",
       "\n",
       "*   Knowledge distillation (KD) allows transferring capabilities from large models to smaller, more efficient ones (IBM, n.d.).\n",
       "*   Quantization reduces model size and computational complexity while maintaining acceptable performance levels (deepchecks, n.d.).\n",
       "*   Active learning focuses on training with the most informative data points, maximizing learning with limited resources (Intuition Labs, n.d.).\n",
       "*   Inverted residuals and linear bottlenecks can reduce computational complexity (lechnowak.com, n.d.).\n",
       "\n",
       "Synthesis: By combining knowledge distillation for model compression, quantization for reduced precision, and active learning for efficient data usage, we can create a fine-tuning methodology that achieves high quality with minimal compute consumption.\n",
       "\n",
       "Logical Reasoning & Inference Chain\n",
       "\n",
       "1.  Evidence: Knowledge distillation transfers knowledge from large to small models (IBM, n.d.). Logical Step: Smaller models require less compute for fine-tuning. Conclusion: KD reduces compute requirements for LLM fine-tuning.\n",
       "2.  Evidence: Quantization reduces model size and improves computing efficiency (deepchecks, n.d.). Logical Step: Smaller models with lower precision require less memory and processing power. Conclusion: Quantization further reduces compute costs.\n",
       "3.  Evidence: Active learning selects the most informative data for training (Intuition Labs, n.d.). Logical Step: Training on fewer, high-value data points reduces the overall training time and resources. Conclusion: Active learning minimizes data labeling and fine-tuning efforts.\n",
       "4.  Evidence: Human-in-the-loop (HITL) training refines models via human feedback (Intuition Labs, n.d.). Logical Step: HITL provides targeted refinement based on preference, improving model alignment and quality. Conclusion: HITL enhances model quality with efficient use of expert input.\n",
       "\n",
       "Proposed Ideas & New Methodologies\n",
       "\n",
       "1.  Name: Compute-Efficient Fine-Tuning via KD, Quantization, and Active Learning\n",
       "    *   Rationale: Combining knowledge distillation, quantization, and active learning maximizes efficiency and quality in LLM fine-tuning (IBM, n.d.; deepchecks, n.d.; Intuition Labs, n.d.).\n",
       "    *   Method:\n",
       "        1.  Distill knowledge from a large, pre-trained LLM to a smaller student model.\n",
       "        2.  Apply quantization techniques to the student model to reduce its size and computational demands.\n",
       "        3.  Implement an active learning loop to strategically select the most informative data points for fine-tuning.\n",
       "        4.  Incorporate human-in-the-loop feedback for preference alignment.\n",
       "        5.  Iteratively fine-tune the quantized student model using the actively selected data.\n",
       "    *   Required inputs: Large pre-trained LLM, unlabeled dataset, human annotators, compute resources for fine-tuning.\n",
       "    *   Success criteria: Reduction in compute cost (e.g., training time, GPU hours), model size, and improved performance on relevant benchmarks compared to baseline.\n",
       "    *   Confidence: Medium. Assumes the distilled model retains sufficient capabilities and active learning effectively identifies informative data points.\n",
       "\n",
       "Implementation Roadmap (phased)\n",
       "\n",
       "*   Phase 0 — Quick wins (0–4 weeks):\n",
       "    *   Implement knowledge distillation using a pre-trained model and a smaller architecture like TinyBERT (lechnowak.com, n.d.).\n",
       "    *   Evaluate the performance of the distilled model on a held-out validation set.\n",
       "*   Phase 1 — Pilot (1–3 months):\n",
       "    *   Implement quantization on the distilled model using dynamic quantization (deepchecks, n.d.).\n",
       "    *   Integrate an active learning strategy with uncertainty sampling.\n",
       "    *   Deliverables: Quantized and actively learned model, active learning query strategy, performance report.\n",
       "    *   Evaluation: Compare performance and compute costs against a baseline fine-tuned model.\n",
       "*   Phase 2 — Scale (3–12 months):\n",
       "    *   Operationalize the fine-tuning pipeline in a cloud environment.\n",
       "    *   Continuously monitor model performance and adjust the active learning strategy as needed.\n",
       "    *   Steps: Automate data selection, expand human annotation, monitor model drift, refine model architecture.\n",
       "\n",
       "Risks & Mitigations\n",
       "\n",
       "*   Risk: Knowledge distillation may lead to loss of crucial information. Mitigation: Carefully select distillation techniques and teacher models. Residual Risk: Medium.\n",
       "*   Risk: Aggressive quantization degrades model accuracy. Mitigation: Use dynamic quantization and evaluate performance. Residual Risk: Low.\n",
       "*   Risk: Active learning fails to identify informative data. Mitigation: Experiment with different sampling strategies. Residual Risk: Medium.\n",
       "*   Risk: Lack of high-quality human annotators. Mitigation: Partner with a reliable annotation service or train in-house annotators. Residual Risk: Low.\n",
       "\n",
       "Cost & Resource Estimate\n",
       "\n",
       "| Resource         | Cost   |\n",
       "| ---------------- | ------ |\n",
       "| Data Acquisition | Low    |\n",
       "| Compute          | Medium |\n",
       "| Personnel        | Medium |\n",
       "\n",
       "KPIs & Dashboard (suggested)\n",
       "\n",
       "| KPI                       | Description                                      | Visualization |\n",
       "| ------------------------- | ------------------------------------------------ | ------------- |\n",
       "| Training Time             | Time taken to fine-tune the model                | Trend         |\n",
       "| GPU Hours                 | GPU usage during fine-tuning                      | Trend         |\n",
       "| Model Size                | Size of the fine-tuned model (MB)               | Bar           |\n",
       "| Benchmark Accuracy        | Performance on relevant benchmarks (e.g., GLUE)  | Bar           |\n",
       "| Data Labeling Cost        | Cost of human annotation for active learning      | Trend         |\n",
       "| Active Learning Query Rate| Rate at which the model queries for human labels | Trend         |\n",
       "\n",
       "Recommendations\n",
       "\n",
       "1.  Prioritize knowledge distillation to reduce model size and computational demands, enabling faster experimentation. Value: Reduced compute costs and faster iteration cycles.\n",
       "2.  Implement dynamic quantization to further reduce model size and improve inference speed without significant accuracy loss. Value: Improved efficiency for deployment on resource-constrained devices.\n",
       "3.  Integrate active learning to focus fine-tuning efforts on the most informative data points, maximizing learning with limited resources. Value: Reduced data labeling costs and improved model performance.\n",
       "\n",
       "Appendix\n",
       "\n",
       "Full Citations\n",
       "\n",
       "*   lechnowak.com. n.d. *Efficient Neural Network Architectures.* Retrieved from [https://lechnowak.com/posts/efficient-neural-network-architectures/](https://lechnowak.com/posts/efficient-neural-network-architectures/)\n",
       "*   Intuition Labs. n.d. *Active Learning (HITL) for LLMs.* Retrieved from [https://intuitionlabs.ai/articles/active-learning-hitl-llms](https://intuitionlabs.ai/articles/active-learning-hitl-llms)\n",
       "*   IBM. n.d. *Knowledge distillation.* Retrieved from [https://www.ibm.com/think/topics/knowledge-distillation](https://www.ibm.com/think/topics/knowledge-distillation)\n",
       "*   Deepchecks. n.d. *Top LLM Quantization Methods and Their Impact on Model Quality.* Retrieved from [https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/](https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print()\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(final_state['final_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59c894-a547-4479-8988-a8d295d16e3b",
   "metadata": {},
   "source": [
    "## Internal Langraph Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60e8941d-b261-41de-a1ad-ed4564710170",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_question': 'How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?How to approach a novel LLM finetuning method with lowest compute consumption possible and highest quality achievable?',\n",
       " 'search_query': '',\n",
       " 'list_of_search_questions': ['What novel neural network architectures facilitate low-compute fine-tuning while maintaining high LLM performance?',\n",
       "  'How can active learning and reinforcement learning be integrated into LLM fine-tuning to minimize data requirements and maximize quality?',\n",
       "  'What are the most effective knowledge distillation techniques for transferring knowledge from large LLMs to smaller, fine-tuned models with minimal performance loss?',\n",
       "  'How do different quantization and pruning methods impact the trade-off between computational cost and accuracy during LLM fine-tuning?',\n",
       "  'What benchmarks and evaluation protocols are best suited to assess the quality and efficiency of novel LLM fine-tuning methods across diverse tasks?',\n",
       "  'What novel neural network architectures facilitate low-compute fine-tuning while maintaining high LLM performance?',\n",
       "  'How can active learning and reinforcement learning be integrated into LLM fine-tuning to minimize data requirements and maximize quality?',\n",
       "  'What are the most effective knowledge distillation techniques for transferring knowledge from large LLMs to smaller, fine-tuned models with minimal performance loss?',\n",
       "  'How do different quantization and pruning methods impact the trade-off between computational cost and accuracy during LLM fine-tuning?',\n",
       "  'What benchmarks and evaluation protocols are best suited to assess the quality and efficiency of novel LLM fine-tuning methods across diverse tasks?'],\n",
       " 'list_of_search_responses': [['Source: https://lechnowak.com/posts/efficient-neural-network-architectures/\\nSnippet: + Inverted Residuals and Linear Bottlenecks: Uses bottleneck layers with inverted residuals to reduce computational complexity while maintaining high performance.\\n Advantages: [...] + Resource-Efficient Deployment: TinyBERT and ALBERT maintain the functionality of their larger counterparts but are optimized for speed and lower resource consumption, making them ideal for real-time and mobile applications.\\n\\n## Conclusion Link to heading [...] # Efficient Neural Network & LLM Architectures\\n\\n9-minute read\\n\\nNeural Networks\\n•\\nDeep Learning\\n\\nNeural Network Architectures\\n•\\nEfficient Models\\n•\\nModel Compression\\n•\\nDeep Learning\\n•\\nLLMs\\n•\\nLLM Optimization\\n•\\nDeep Learning Optimization',\n",
       "   \"Source: https://www.e2enetworks.com/blog/retentive-network-a-novel-neural-network-architecture\\nSnippet: 6 min read\\n### How Does RAG Improve the Accuracy of LLM Responses?\\n\\n## Build on the most powerful infrastructure cloud\\n\\nContact Sales\\nGet Started for Free\\n\\nWe're Online! [...] August 20, 2025\\n\\n4 min read\\n### Making AI Deployment Affordable and Scalable: Cost Efficiency of Quantization\\n\\nAugust 18, 2025\\n\\n7 min read\\n### Interpretable vs. Black-Box Models: A Comprehensive Exploration on Early Prediction under Uncertainty\\n\\nAugust 8, 2025\\n\\n10 min read\\n### Generative AI in Healthcare: Applications, Benefits, and Its Future\\n\\nAugust 7, 2025\\n\\n6 mins read\\n### No-Code Deployment of Fine-Tuned Models on TIR Foundation Studio: BYOM Made Easy\\n\\nAugust 1, 2025\"],\n",
       "  ['Source: https://intuitionlabs.ai/articles/active-learning-hitl-llms\\nSnippet: Looking ahead, we can expect more seamless integration of active learning in LLM development. For instance, future LLMs might have built-in uncertainty estimators or explanations for when to ask for help, making the human-proactive querying more natural. We may also see meta-active-learning, where models learn how to actively learn (optimizing their own query strategies via reinforcement learning or meta-learning). Foundation model research suggests that new strategies, perhaps learned [...] Figure 1: A typical human-in-the-loop training pipeline for aligning an LLM, consisting of supervised fine-tuning on human-written demonstrations (to get an initial aligned model), followed by iterative preference collection and reinforcement learning from human feedback to further refine the model. In essence, a pretrained model (“untamed monster”) is made more helpful and safe through fine-tuning on quality data, then “given a smiley face” via RLHF using human preference comparisons [...] RL Fine-Tuning: Finally, the original LLM is fine-tuned using reinforcement learning (commonly Proximal Policy Optimization, PPO) to maximize the reward model’s score. Essentially, the LLM acts as a policy generating text, and the reward model’s output is treated as the reward signal to optimize. Over many episodes, the LLM learns to produce outputs that the reward model (and thus, presumably humans) favor. This stage is where the term \"RLHF\" is most literal: the human feedback (via the RM) is',\n",
       "   'Source: https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/\\nSnippet: With this in mind, we describe a new, scalable curation process for active learning) that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.'],\n",
       "  ['Source: https://www.ibm.com/think/topics/knowledge-distillation\\nSnippet: Knowledge distillation techniques have since been successfully employed across diverse fields, including natural language processing (NLP), speech recognition, image recognition and object detection. In recent years, the study of knowledge distillation has been of particular importance to large language models (LLMs). For LLMs, KD has emerged as an effective means of transferring advanced capabilities from leading proprietary models to smaller, more accessible open source models. [...] Furthermore, most commercially viable LLMs are too large and computationally demanding to be used locally on mobile phones or other edge devices. This presents various logistical, computational and privacy complications that would otherwise be circumvented with a smaller model that could be run directly on mobile devices. KD’s model compression thus presents a promising means to transfer the emergent qualities of large models to models small enough to be run on-device. [...] Using larger, proprietary LLMs to generate data sets for the instruction tuning of smaller models. For example, Microsoft’s Orca model “learn(ed) from rich signals from GPT-4 including explanation traces, step-by-step thought processes and other complex instructions.” 10',\n",
       "   \"Source: https://zilliz.com/learn/knowledge-distillation-from-large-language-models-deep-dive\\nSnippet: Sustainability: KD contributes to AI's environmental sustainability by enabling smaller, more efficient models that require less computational power.\\n Model Compression and Efficiency: KD enables the development of smaller, efficient LLMs with minimal performance loss, essential for AI deployment in resource-limited environments like mobile devices. [...] Mastering Cohere's Reranker for Enhanced AI Performance\\n Efficient Memory Management for Large Language Model Serving with PagedAttention\\n LoRA Explained: Low-Rank Adaptation for Fine-Tuning LLMs\\n Knowledge Distillation: Transferring Knowledge from Large, Computationally Expensive LLMs to Smaller Ones Without Sacrificing Validity\\n RouteLLM: An Open-Source Framework for Navigating Cost-Quality Trade-Offs in LLM Deployment\\n Prover-Verifier Games Improve Legibility of LLM Outputs [...] Knowledge Distillation (KD) is an important technique for bridging this gap. Through distillation techniques, the gap between proprietary LLMs and open-source models is greatly reduced and even filled. Knowledge distillation is important for improving the capabilities of LLMs, model compression, and self-improvement. Additionally, it helps models act as their own teachers for self-improvement.\"],\n",
       "  ['Source: https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/\\nSnippet: # Top LLM Quantization Methods and Their Impact on Model Quality LLM quantization is a method for reducing the precision of weights and activations in LLMs. The goal is to lower the model’s size and improve its computing efficiency without sacrificing too much performance. * **Impact on model quality:** Dynamic quantization achieves minimal performance degradation by using lower-precision formats only during inference, as opposed to static quantization, which pre-quantifies all weights and activations based on fixed scales. Although binary and ternary quantization techniques can reduce model size and computational complexity, the drop in accuracy is more significant than that of higher precision quantization. ## Top LLM Quantization Methods and Their Impact on Model Quality ## Top LLM Quantization Methods and Their Impact on Model Quality',\n",
       "   'Source: https://www.amazon.science/blog/a-better-path-to-pruning-large-language-models\\nSnippet: Key job responsibilities - Collaborate with AI/ML scientists, engineers, and architects to research, design, develop, and evaluate cutting-edge generative AI algorithms and build ML systems to address real-world challenges - Interact with customers directly to understand the business problem, help and aid them in implementation of generative AI solutions, deliver briefing and deep dive sessions to customers and guide customer on adoption patterns and paths to production - Create and deliver best practice recommendations, tutorials, blog posts, publications, sample code, and presentations adapted to technical, business, and executive stakeholder - Provide customer and market feedback to Product and Engineering teams to help define product direction About the team Diverse Experiences Amazon values diverse experiences.'],\n",
       "  ['Source: https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5\\nSnippet: Table 1: Sample LLM model evaluation benchmarks\\n\\n## Evaluation frameworks and platforms\\n\\nIt is imperative to assess LLMs to gauge their quality and efficacy across diverse applications. Numerous frameworks have been devised specifically for the evaluation of LLMs. Below, we highlight some of the most widely recognized ones, such as Prompt Flow in Microsoft Azure AI studio, Weights & Biases in combination of LangChain, LangSmith by LangChain, DeepEval by confidence-ai, TruEra, and more. [...] While this article focuses on the evaluation of LLM systems, it is crucial to discern the difference between assessing a standalone Large Language Model (LLM) and evaluating an LLM-based system. Today’s LLMs exhibit versatility by performing various tasks such as Chatbot, Named Entity Recognition (NER), text generation, summarization, question-answering, sentiment analysis, translation, and more. Typically, these models undergo evaluation on standardized benchmarks in Table 1 such as GLUE',\n",
       "   'Source: https://www.evidentlyai.com/llm-guide/llm-benchmarks\\nSnippet: LLM benchmarksare standardized tests that assess LLM performance across various tasks. Typically, they check if the model can produce the correct known response to a given input.\\n   Common LLM benchmarks test models for skills like language understanding, question-answering, math problem-solving, and coding tasks. Examplesare HellaSwag, BigBench, TruthfulQA, and Chatbot Arena. [...] Once the model completes the benchmark tasks, you can measure its quality! Each benchmark includes a scoring mechanism to quantify how well an LLM performs, with different evaluation methods suited to different task types. Here are some examples: [...] We can already see a history where certain benchmarks became outdated as models consistently surpassed them, pushing researchers to develop more challenging benchmarks to keep up with advanced LLM capabilities.\\n\\nYou can also use benchmarks to identify the model’s weak spots. For instance, a safety benchmark can show how well a given LLM handles novel threats. This, in turn, guides the fine-tuning process and helps LLM researchers advance the field.'],\n",
       "  ['Source: https://lechnowak.com/posts/efficient-neural-network-architectures/\\nSnippet: + Inverted Residuals and Linear Bottlenecks: Uses bottleneck layers with inverted residuals to reduce computational complexity while maintaining high performance.\\n Advantages: [...] + Resource-Efficient Deployment: TinyBERT and ALBERT maintain the functionality of their larger counterparts but are optimized for speed and lower resource consumption, making them ideal for real-time and mobile applications.\\n\\n## Conclusion Link to heading [...] # Efficient Neural Network & LLM Architectures\\n\\n9-minute read\\n\\nNeural Networks\\n•\\nDeep Learning\\n\\nNeural Network Architectures\\n•\\nEfficient Models\\n•\\nModel Compression\\n•\\nDeep Learning\\n•\\nLLMs\\n•\\nLLM Optimization\\n•\\nDeep Learning Optimization',\n",
       "   \"Source: https://www.e2enetworks.com/blog/retentive-network-a-novel-neural-network-architecture\\nSnippet: 6 min read\\n### How Does RAG Improve the Accuracy of LLM Responses?\\n\\n## Build on the most powerful infrastructure cloud\\n\\nContact Sales\\nGet Started for Free\\n\\nWe're Online! [...] August 20, 2025\\n\\n4 min read\\n### Making AI Deployment Affordable and Scalable: Cost Efficiency of Quantization\\n\\nAugust 18, 2025\\n\\n7 min read\\n### Interpretable vs. Black-Box Models: A Comprehensive Exploration on Early Prediction under Uncertainty\\n\\nAugust 8, 2025\\n\\n10 min read\\n### Generative AI in Healthcare: Applications, Benefits, and Its Future\\n\\nAugust 7, 2025\\n\\n6 mins read\\n### No-Code Deployment of Fine-Tuned Models on TIR Foundation Studio: BYOM Made Easy\\n\\nAugust 1, 2025\"],\n",
       "  ['Source: https://intuitionlabs.ai/articles/active-learning-hitl-llms\\nSnippet: Looking ahead, we can expect more seamless integration of active learning in LLM development. For instance, future LLMs might have built-in uncertainty estimators or explanations for when to ask for help, making the human-proactive querying more natural. We may also see meta-active-learning, where models learn how to actively learn (optimizing their own query strategies via reinforcement learning or meta-learning). Foundation model research suggests that new strategies, perhaps learned [...] Figure 1: A typical human-in-the-loop training pipeline for aligning an LLM, consisting of supervised fine-tuning on human-written demonstrations (to get an initial aligned model), followed by iterative preference collection and reinforcement learning from human feedback to further refine the model. In essence, a pretrained model (“untamed monster”) is made more helpful and safe through fine-tuning on quality data, then “given a smiley face” via RLHF using human preference comparisons [...] RL Fine-Tuning: Finally, the original LLM is fine-tuned using reinforcement learning (commonly Proximal Policy Optimization, PPO) to maximize the reward model’s score. Essentially, the LLM acts as a policy generating text, and the reward model’s output is treated as the reward signal to optimize. Over many episodes, the LLM learns to produce outputs that the reward model (and thus, presumably humans) favor. This stage is where the term \"RLHF\" is most literal: the human feedback (via the RM) is',\n",
       "   'Source: https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/\\nSnippet: With this in mind, we describe a new, scalable curation process for active learning) that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.'],\n",
       "  ['Source: https://www.ibm.com/think/topics/knowledge-distillation\\nSnippet: Knowledge distillation techniques have since been successfully employed across diverse fields, including natural language processing (NLP), speech recognition, image recognition and object detection. In recent years, the study of knowledge distillation has been of particular importance to large language models (LLMs). For LLMs, KD has emerged as an effective means of transferring advanced capabilities from leading proprietary models to smaller, more accessible open source models. [...] Furthermore, most commercially viable LLMs are too large and computationally demanding to be used locally on mobile phones or other edge devices. This presents various logistical, computational and privacy complications that would otherwise be circumvented with a smaller model that could be run directly on mobile devices. KD’s model compression thus presents a promising means to transfer the emergent qualities of large models to models small enough to be run on-device. [...] Using larger, proprietary LLMs to generate data sets for the instruction tuning of smaller models. For example, Microsoft’s Orca model “learn(ed) from rich signals from GPT-4 including explanation traces, step-by-step thought processes and other complex instructions.” 10',\n",
       "   \"Source: https://zilliz.com/learn/knowledge-distillation-from-large-language-models-deep-dive\\nSnippet: Sustainability: KD contributes to AI's environmental sustainability by enabling smaller, more efficient models that require less computational power.\\n Model Compression and Efficiency: KD enables the development of smaller, efficient LLMs with minimal performance loss, essential for AI deployment in resource-limited environments like mobile devices. [...] Mastering Cohere's Reranker for Enhanced AI Performance\\n Efficient Memory Management for Large Language Model Serving with PagedAttention\\n LoRA Explained: Low-Rank Adaptation for Fine-Tuning LLMs\\n Knowledge Distillation: Transferring Knowledge from Large, Computationally Expensive LLMs to Smaller Ones Without Sacrificing Validity\\n RouteLLM: An Open-Source Framework for Navigating Cost-Quality Trade-Offs in LLM Deployment\\n Prover-Verifier Games Improve Legibility of LLM Outputs [...] Knowledge Distillation (KD) is an important technique for bridging this gap. Through distillation techniques, the gap between proprietary LLMs and open-source models is greatly reduced and even filled. Knowledge distillation is important for improving the capabilities of LLMs, model compression, and self-improvement. Additionally, it helps models act as their own teachers for self-improvement.\"],\n",
       "  ['Source: https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/\\nSnippet: # Top LLM Quantization Methods and Their Impact on Model Quality LLM quantization is a method for reducing the precision of weights and activations in LLMs. The goal is to lower the model’s size and improve its computing efficiency without sacrificing too much performance. * **Impact on model quality:** Dynamic quantization achieves minimal performance degradation by using lower-precision formats only during inference, as opposed to static quantization, which pre-quantifies all weights and activations based on fixed scales. Although binary and ternary quantization techniques can reduce model size and computational complexity, the drop in accuracy is more significant than that of higher precision quantization. ## Top LLM Quantization Methods and Their Impact on Model Quality ## Top LLM Quantization Methods and Their Impact on Model Quality',\n",
       "   'Source: https://www.amazon.science/blog/a-better-path-to-pruning-large-language-models\\nSnippet: Key job responsibilities - Collaborate with AI/ML scientists, engineers, and architects to research, design, develop, and evaluate cutting-edge generative AI algorithms and build ML systems to address real-world challenges - Interact with customers directly to understand the business problem, help and aid them in implementation of generative AI solutions, deliver briefing and deep dive sessions to customers and guide customer on adoption patterns and paths to production - Create and deliver best practice recommendations, tutorials, blog posts, publications, sample code, and presentations adapted to technical, business, and executive stakeholder - Provide customer and market feedback to Product and Engineering teams to help define product direction About the team Diverse Experiences Amazon values diverse experiences.'],\n",
       "  ['Source: https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5\\nSnippet: Table 1: Sample LLM model evaluation benchmarks\\n\\n## Evaluation frameworks and platforms\\n\\nIt is imperative to assess LLMs to gauge their quality and efficacy across diverse applications. Numerous frameworks have been devised specifically for the evaluation of LLMs. Below, we highlight some of the most widely recognized ones, such as Prompt Flow in Microsoft Azure AI studio, Weights & Biases in combination of LangChain, LangSmith by LangChain, DeepEval by confidence-ai, TruEra, and more. [...] While this article focuses on the evaluation of LLM systems, it is crucial to discern the difference between assessing a standalone Large Language Model (LLM) and evaluating an LLM-based system. Today’s LLMs exhibit versatility by performing various tasks such as Chatbot, Named Entity Recognition (NER), text generation, summarization, question-answering, sentiment analysis, translation, and more. Typically, these models undergo evaluation on standardized benchmarks in Table 1 such as GLUE',\n",
       "   'Source: https://www.evidentlyai.com/llm-guide/llm-benchmarks\\nSnippet: LLM benchmarksare standardized tests that assess LLM performance across various tasks. Typically, they check if the model can produce the correct known response to a given input.\\n   Common LLM benchmarks test models for skills like language understanding, question-answering, math problem-solving, and coding tasks. Examplesare HellaSwag, BigBench, TruthfulQA, and Chatbot Arena. [...] Once the model completes the benchmark tasks, you can measure its quality! Each benchmark includes a scoring mechanism to quantify how well an LLM performs, with different evaluation methods suited to different task types. Here are some examples: [...] We can already see a history where certain benchmarks became outdated as models consistently surpassed them, pushing researchers to develop more challenging benchmarks to keep up with advanced LLM capabilities.\\n\\nYou can also use benchmarks to identify the model’s weak spots. For instance, a safety benchmark can show how well a given LLM handles novel threats. This, in turn, guides the fine-tuning process and helps LLM researchers advance the field.']],\n",
       " 'log_messages': [],\n",
       " 'final_summary': 'Title: Efficient LLM Fine-tuning for Resource-Constrained Environments\\n\\nPurpose: This report proposes a novel, compute-efficient LLM fine-tuning methodology to achieve high model quality, focusing on techniques suitable for resource-constrained environments.\\n\\nExecutive Summary\\n\\n*   Combining knowledge distillation (IBM, n.d.), quantization (deepchecks, n.d.), and active learning (Intuition Labs, n.d.) offers a path to efficient and effective LLM fine-tuning.\\n*   Knowledge distillation enables transferring knowledge from large models to smaller ones, reducing computational demands.\\n*   Quantization reduces model size and improves computing efficiency with minimal performance degradation (deepchecks, n.d.).\\n*   Active learning optimizes data usage by strategically selecting the most informative samples for human labeling and fine-tuning (Intuition Labs, n.d.).\\n*   We recommend a phased implementation, starting with knowledge distillation, followed by quantization and active learning integration, to incrementally improve efficiency and quality.\\n\\nKey Evidence & Synthesis\\n\\n*   Knowledge distillation (KD) allows transferring capabilities from large models to smaller, more efficient ones (IBM, n.d.).\\n*   Quantization reduces model size and computational complexity while maintaining acceptable performance levels (deepchecks, n.d.).\\n*   Active learning focuses on training with the most informative data points, maximizing learning with limited resources (Intuition Labs, n.d.).\\n*   Inverted residuals and linear bottlenecks can reduce computational complexity (lechnowak.com, n.d.).\\n\\nSynthesis: By combining knowledge distillation for model compression, quantization for reduced precision, and active learning for efficient data usage, we can create a fine-tuning methodology that achieves high quality with minimal compute consumption.\\n\\nLogical Reasoning & Inference Chain\\n\\n1.  Evidence: Knowledge distillation transfers knowledge from large to small models (IBM, n.d.). Logical Step: Smaller models require less compute for fine-tuning. Conclusion: KD reduces compute requirements for LLM fine-tuning.\\n2.  Evidence: Quantization reduces model size and improves computing efficiency (deepchecks, n.d.). Logical Step: Smaller models with lower precision require less memory and processing power. Conclusion: Quantization further reduces compute costs.\\n3.  Evidence: Active learning selects the most informative data for training (Intuition Labs, n.d.). Logical Step: Training on fewer, high-value data points reduces the overall training time and resources. Conclusion: Active learning minimizes data labeling and fine-tuning efforts.\\n4.  Evidence: Human-in-the-loop (HITL) training refines models via human feedback (Intuition Labs, n.d.). Logical Step: HITL provides targeted refinement based on preference, improving model alignment and quality. Conclusion: HITL enhances model quality with efficient use of expert input.\\n\\nProposed Ideas & New Methodologies\\n\\n1.  Name: Compute-Efficient Fine-Tuning via KD, Quantization, and Active Learning\\n    *   Rationale: Combining knowledge distillation, quantization, and active learning maximizes efficiency and quality in LLM fine-tuning (IBM, n.d.; deepchecks, n.d.; Intuition Labs, n.d.).\\n    *   Method:\\n        1.  Distill knowledge from a large, pre-trained LLM to a smaller student model.\\n        2.  Apply quantization techniques to the student model to reduce its size and computational demands.\\n        3.  Implement an active learning loop to strategically select the most informative data points for fine-tuning.\\n        4.  Incorporate human-in-the-loop feedback for preference alignment.\\n        5.  Iteratively fine-tune the quantized student model using the actively selected data.\\n    *   Required inputs: Large pre-trained LLM, unlabeled dataset, human annotators, compute resources for fine-tuning.\\n    *   Success criteria: Reduction in compute cost (e.g., training time, GPU hours), model size, and improved performance on relevant benchmarks compared to baseline.\\n    *   Confidence: Medium. Assumes the distilled model retains sufficient capabilities and active learning effectively identifies informative data points.\\n\\nImplementation Roadmap (phased)\\n\\n*   Phase 0 — Quick wins (0–4 weeks):\\n    *   Implement knowledge distillation using a pre-trained model and a smaller architecture like TinyBERT (lechnowak.com, n.d.).\\n    *   Evaluate the performance of the distilled model on a held-out validation set.\\n*   Phase 1 — Pilot (1–3 months):\\n    *   Implement quantization on the distilled model using dynamic quantization (deepchecks, n.d.).\\n    *   Integrate an active learning strategy with uncertainty sampling.\\n    *   Deliverables: Quantized and actively learned model, active learning query strategy, performance report.\\n    *   Evaluation: Compare performance and compute costs against a baseline fine-tuned model.\\n*   Phase 2 — Scale (3–12 months):\\n    *   Operationalize the fine-tuning pipeline in a cloud environment.\\n    *   Continuously monitor model performance and adjust the active learning strategy as needed.\\n    *   Steps: Automate data selection, expand human annotation, monitor model drift, refine model architecture.\\n\\nRisks & Mitigations\\n\\n*   Risk: Knowledge distillation may lead to loss of crucial information. Mitigation: Carefully select distillation techniques and teacher models. Residual Risk: Medium.\\n*   Risk: Aggressive quantization degrades model accuracy. Mitigation: Use dynamic quantization and evaluate performance. Residual Risk: Low.\\n*   Risk: Active learning fails to identify informative data. Mitigation: Experiment with different sampling strategies. Residual Risk: Medium.\\n*   Risk: Lack of high-quality human annotators. Mitigation: Partner with a reliable annotation service or train in-house annotators. Residual Risk: Low.\\n\\nCost & Resource Estimate\\n\\n| Resource         | Cost   |\\n| ---------------- | ------ |\\n| Data Acquisition | Low    |\\n| Compute          | Medium |\\n| Personnel        | Medium |\\n\\nKPIs & Dashboard (suggested)\\n\\n| KPI                       | Description                                      | Visualization |\\n| ------------------------- | ------------------------------------------------ | ------------- |\\n| Training Time             | Time taken to fine-tune the model                | Trend         |\\n| GPU Hours                 | GPU usage during fine-tuning                      | Trend         |\\n| Model Size                | Size of the fine-tuned model (MB)               | Bar           |\\n| Benchmark Accuracy        | Performance on relevant benchmarks (e.g., GLUE)  | Bar           |\\n| Data Labeling Cost        | Cost of human annotation for active learning      | Trend         |\\n| Active Learning Query Rate| Rate at which the model queries for human labels | Trend         |\\n\\nRecommendations\\n\\n1.  Prioritize knowledge distillation to reduce model size and computational demands, enabling faster experimentation. Value: Reduced compute costs and faster iteration cycles.\\n2.  Implement dynamic quantization to further reduce model size and improve inference speed without significant accuracy loss. Value: Improved efficiency for deployment on resource-constrained devices.\\n3.  Integrate active learning to focus fine-tuning efforts on the most informative data points, maximizing learning with limited resources. Value: Reduced data labeling costs and improved model performance.\\n\\nAppendix\\n\\nFull Citations\\n\\n*   lechnowak.com. n.d. *Efficient Neural Network Architectures.* Retrieved from [https://lechnowak.com/posts/efficient-neural-network-architectures/](https://lechnowak.com/posts/efficient-neural-network-architectures/)\\n*   Intuition Labs. n.d. *Active Learning (HITL) for LLMs.* Retrieved from [https://intuitionlabs.ai/articles/active-learning-hitl-llms](https://intuitionlabs.ai/articles/active-learning-hitl-llms)\\n*   IBM. n.d. *Knowledge distillation.* Retrieved from [https://www.ibm.com/think/topics/knowledge-distillation](https://www.ibm.com/think/topics/knowledge-distillation)\\n*   Deepchecks. n.d. *Top LLM Quantization Methods and Their Impact on Model Quality.* Retrieved from [https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/](https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/)\\n'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8cd785-c8dd-41b8-8b06-b45c996d6c88",
   "metadata": {},
   "source": [
    "## Reason and Action  - What already aligns with this method\n",
    "\n",
    "Explicit reasoning step — get_search_questions uses an LLM to reason about the user query and generate a list of targeted search questions. (This is the agent “thought” / planning phase.)\n",
    "\n",
    "Action step (tool call) — search_tool iterates over those questions and calls tavily_client.search(...) for each, i.e., the notebook executes external actions (web search) based on the LLM-produced plan.\n",
    "\n",
    "Synthesis step — summary uses an LLM to combine search results into a final executive summary. This is the agent’s synthesis/conclusion after actions and evidence collection.\n",
    "\n",
    "StateGraph orchestration — The pipeline is organized as nodes (get_search_questions → search_tool → summary) and executed via StateGraph which enforces a flow: plan → act → summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1e298-9a05-4c62-8416-44b096a699d4",
   "metadata": {},
   "source": [
    "## ReAct Method applied to improve final summary generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f299564-b70f-4f60-855d-6ae5e0f9413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_call(prompt) -> str:\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    response = model.generate_content(\n",
    "        prompt\n",
    "    )\n",
    "    \n",
    "    text = response.candidates[0].content.parts[0].text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe27c1ae-418a-4e3d-b2f8-33c1a045a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Callable, List, Dict, Tuple, Optional\n",
    "\n",
    "JSON_RE = re.compile(r\"\\{(?:.|\\n)*\\}\", re.DOTALL)\n",
    "\n",
    "def _extract_json(text: str) -> Optional[dict]:\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        m = JSON_RE.search(text)\n",
    "        if not m:\n",
    "            return None\n",
    "        s = m.group(0)\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return json.loads(s.replace(\"'\", '\"'))\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "\n",
    "def _build_prompt(current_summary: str, recent_critique: Optional[str] = None) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert editor and methodological reasoner (Gemini).\\n\"\n",
    "        \"Follow a strict Thought -> Action -> ActionInput JSON protocol. Return JSON ONLY.\\n\\n\"\n",
    "        \"Required JSON keys:\\n\"\n",
    "        \"  - thought: one-sentence reasoning about next step\\n\"\n",
    "        \"  - action: one of [revise, critique, restructure, expand, finish]\\n\"\n",
    "        \"  - action_input: the instruction or text the agent will use to perform the action\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"CURRENT_SUMMARY:\\n{current_summary}\\n\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- No external web searches or data fetches; rely only on the text provided and general knowledge.\\n\"\n",
    "        \"- If a claim requires external evidence, produce a clear short note specifying the exact evidence to obtain and mark confidence as Low.\\n\\n\"\n",
    "        \"Goal:\\n\"\n",
    "        \"Iteratively improve clarity, structure, and usefulness for executives: increase precision, reduce ambiguity, and surface assumptions and confidence levels.\\n\\n\"\n",
    "    )\n",
    "    if recent_critique:\n",
    "        prompt += f\"Recent critique notes:\\n{recent_critique}\\n\\n\"\n",
    "    prompt += (\n",
    "        \"Examples of valid JSON:\\n\"\n",
    "        \"{\" + '\"thought\": \"Summary lacks evidence for claim X\", \"action\": \"revise\", \"action_input\": \"Reword claim X to a hedged phrasing and list assumptions\"' + \"}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def gemini_reviser(\n",
    "    initial_summary: str,\n",
    "    gemini_call: Callable[[str], str],\n",
    "    max_steps: int = 6,\n",
    "    temperature: float = 0.0,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[str, List[Dict[str, str]]]:\n",
    "\n",
    "    summary = initial_summary\n",
    "    history: List[Dict[str, str]] = []\n",
    "    recent_critique = None\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        prompt = _build_prompt(summary, recent_critique)\n",
    "        raw = gemini_call(prompt)\n",
    "        parsed = _extract_json(raw)\n",
    "\n",
    "        if parsed is None:\n",
    "            # Fallback: ask Gemini to produce a direct revised summary\n",
    "            fallback_prompt = (\n",
    "                \"You failed to return JSON. Ignore the protocol and simply return an improved executive summary.\\n\"\n",
    "                \"Current summary:\\n\" + summary + \"\\n\\nReturn only the revised summary text.\"\n",
    "            )\n",
    "            revised = gemini_call(fallback_prompt)\n",
    "            history.append({\n",
    "                \"step\": step,\n",
    "                \"thought\": \"PARSE_FAILED\",\n",
    "                \"action\": \"revise_fallback\",\n",
    "                \"action_input\": \"\",\n",
    "                \"summary_before\": summary,\n",
    "                \"summary_after\": revised,\n",
    "            })\n",
    "            summary = revised\n",
    "            recent_critique = None\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: parse failed -> applied fallback revision\")\n",
    "            continue\n",
    "\n",
    "        thought = parsed.get(\"thought\", \"\")\n",
    "        action = parsed.get(\"action\", \"revise\").lower()\n",
    "        action_input = parsed.get(\"action_input\", \"\")\n",
    "\n",
    "        summary_before = summary\n",
    "\n",
    "        if action == \"finish\":\n",
    "            history.append({\n",
    "                \"step\": step,\n",
    "                \"thought\": thought,\n",
    "                \"action\": action,\n",
    "                \"action_input\": action_input,\n",
    "                \"summary_before\": summary_before,\n",
    "                \"summary_after\": summary_before,\n",
    "            })\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: finish requested. Ending loop.\")\n",
    "            break\n",
    "\n",
    "        if action == \"critique\":\n",
    "            # action_input may be a focused instruction for critique; otherwise produce critique\n",
    "            critique_prompt = (\n",
    "                \"You are a critical reviewer. Provide a concise numbered list of weaknesses, ambiguous claims, and assumptions in the summary.\\n\\n\"\n",
    "                \"Summary:\\n\" + summary + \"\\n\\nReturn only the critique list (numbered).\"\n",
    "            )\n",
    "            critique_text = gemini_call(critique_prompt)\n",
    "            recent_critique = critique_text\n",
    "            summary_after = summary\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: critique produced\")\n",
    "\n",
    "        elif action == \"revise\":\n",
    "            # Use action_input as revision instruction\n",
    "            revise_instruction = action_input or \"Improve clarity, remove ambiguity, for executives.\"\n",
    "            revise_prompt = (\n",
    "                \"You are an expert executive editor and critical reasoner (Gemini). \"\n",
    "                \"Your goal is to *revise and enhance* the given executive report according to the instruction — \"\n",
    "                \"but DO NOT shorten or condense it.\\n\\n\"\n",
    "                \"Enhancement Rules:\\n\"\n",
    "                \"1. Maintain the same overall length and paragraph structure as the original.\\n\"\n",
    "                \"2. Improve logical flow, precision, and clarity without losing any information.\\n\"\n",
    "                \"3. Expand slightly (≤10%) only when adding short clarifications, transitions, or actionable insights explicitly implied in the text.\\n\"\n",
    "                \"4. Preserve all technical and contextual details; never omit data, reasoning, or examples.\\n\"\n",
    "                \"5. After each modified section or paragraph, append a short bracketed note like \"\n",
    "                \"[edit_reason; confidence]. Example: [clarified causal link; High].\\n\"\n",
    "                \"6. If the instruction asks for stylistic or focus changes, apply them *without reducing factual depth*.\\n\"\n",
    "                \"7. End with a short 'Revision Notes' section summarizing key changes and why they improve reasoning quality.\\n\\n\"\n",
    "                \"Instruction:\\n\" + revise_instruction + \"\\n\\n\"\n",
    "                \"Current executive report:\\n\" + summary + \"\\n\\n\"\n",
    "                \"Return only the *fully revised enhanced text* (with bracketed notes and a final 'Revision Notes' section).\"\n",
    "            )\n",
    "            revised = gemini_call(revise_prompt)\n",
    "            summary = revised\n",
    "            summary_after = summary\n",
    "            recent_critique = None\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: revise applied\")\n",
    "\n",
    "        elif action == \"restructure\":\n",
    "            # Reorder or add headings\n",
    "            restructure_instruction = action_input or \"Reorder content into clear headings: Title, Purpose, Key Findings, Recommendations.\"\n",
    "            restructure_prompt = (\n",
    "                \"You are an executive report formatter. Re-structure the summary into clear sections as instructed.\\n\\n\"\n",
    "                \"Instruction:\\n\" + restructure_instruction + \"\\n\\n\"\n",
    "                \"Current summary:\\n\" + summary + \"\\n\\nReturn only the re-structured summary text.\"\n",
    "            )\n",
    "            summary = gemini_call(restructure_prompt)\n",
    "            summary_after = summary\n",
    "            recent_critique = None\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: restructure applied\")\n",
    "\n",
    "        elif action == \"expand\":\n",
    "            expand_instruction = action_input or \"Expand the summary with a short methodology paragraph and 2 recommended next steps.\"\n",
    "            expand_prompt = (\n",
    "                \"You are an analyst. Expand the summary according to the instruction.\\n\\n\"\n",
    "                \"Instruction:\\n\" + expand_instruction + \"\\n\\n\"\n",
    "                \"Current summary:\\n\" + summary + \"\\n\\nReturn only the expanded summary text.\"\n",
    "            )\n",
    "            summary = gemini_call(expand_prompt)\n",
    "            summary_after = summary\n",
    "            recent_critique = None\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: expand applied\")\n",
    "\n",
    "        else:\n",
    "            # Unknown action: treat as a revise instruction\n",
    "            fallback_revise = action_input or \"Improve clarity and executive focus.\"\n",
    "            fallback_prompt = (\n",
    "                \"You are an expert editor. Revise the executive summary according to the instruction.\\n\\n\"\n",
    "                \"Instruction:\\n\" + fallback_revise + \"\\n\\n\"\n",
    "                \"Current summary:\\n\" + summary + \"\\n\\nReturn only the revised summary text.\"\n",
    "            )\n",
    "            summary = gemini_call(fallback_prompt)\n",
    "            summary_after = summary\n",
    "            recent_critique = None\n",
    "            if verbose:\n",
    "                print(f\"Step {step}: unknown action fallback revise applied\")\n",
    "\n",
    "        history.append({\n",
    "            \"step\": step,\n",
    "            \"thought\": thought,\n",
    "            \"action\": action,\n",
    "            \"action_input\": action_input,\n",
    "            \"summary_before\": summary_before,\n",
    "            \"summary_after\": summary_after,\n",
    "        })\n",
    "\n",
    "    # Final condensation pass to ensure executive-friendly output\n",
    "    final_enhance_prompt = (\n",
    "        \"You are an expert executive editor and domain synthesizer (Gemini). \"\n",
    "        \"DO NOT shorten or overly-condense the text. Your goal is to ENHANCE the provided executive report by:\\n\"\n",
    "        \"  - improving clarity, flow, and professional wording,\\n\"\n",
    "        \"  - making implicit assumptions explicit,\\n\"\n",
    "        \"  - adding brief, actionable improvements or methodological notes where helpful,\\n\"\n",
    "        \"  - labeling any claims that lack external evidence and stating what exact evidence would be needed,\\n\"\n",
    "        \"  - marking confidence levels for major claims (High / Medium / Low).\\n\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"  1) Preserve the length and detail of the report; do not reduce core paragraphs. You may expand small sections (up to ~20%) only to add clarifying detail or methodology suggestions.\\n\"\n",
    "        \"  2) Keep original headings and structure unless you explicitly improve them while retaining all content.\\n\"\n",
    "        \"  3) For each substantive paragraph or claim you modify, append a short bracketed note indicating: [edit_reason; confidence].\\n\"\n",
    "        \"  4) At the end, include an 'Assumptions & Evidence Needed' section listing exact datasets/metrics to verify claims (one-line per item).\\n\\n\"\n",
    "        \"Current report:\\n\" + summary + \"\\n\\n\"\n",
    "        \"Return the full enhanced report text only (with the brief bracketed notes and the final 'Assumptions & Evidence Needed' section).\"\n",
    ")\n",
    "    enhanced = gemini_call(final_enhance_prompt)\n",
    "    history.append({\"step\": \"finalize\", \"result\": enhanced})\n",
    "\n",
    "    return enhanced, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "940bd4a9-9f23-4d4b-9213-56b5c5276c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: revise applied\n",
      "Step 2: restructure applied\n",
      "Step 3: revise applied\n",
      "Step 4: expand applied\n"
     ]
    }
   ],
   "source": [
    "initial_summary_text = final_state['final_summary']\n",
    "final_summary, history = gemini_reviser(\n",
    "    initial_summary=initial_summary_text,\n",
    "    gemini_call=gemini_call,\n",
    "    max_steps=4,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6c66b-e636-482f-88be-89a4a6780bca",
   "metadata": {},
   "source": [
    "## Reason, thought and action described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "69396056-dba4-49b3-a8d8-60cc96463b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Step 1\n",
      "  💭 Thought      : The executive summary could be strengthened by explicitly stating the problem being solved.\n",
      "  ⚙️  Action       : revise\n",
      "  📝 Action Input : Add a sentence at the beginning of the executive summary that explicitly states the problem being addressed: 'Fine-tuning large language models (LLMs) typically demands significant computational resources, posing a challenge for organizations with limited infrastructure.'\n",
      "------------------------------------------------------------\n",
      "🧩 Step 2\n",
      "  💭 Thought      : The report is comprehensive but could benefit from a more visually appealing and informative format, particularly in the 'Implementation Roadmap' and 'KPIs & Dashboard' sections. Consider converting these sections into tables or diagrams for better readability.\n",
      "  ⚙️  Action       : restructure\n",
      "  📝 Action Input : Convert the 'Implementation Roadmap' and 'KPIs & Dashboard' sections into tabular formats.\n",
      "------------------------------------------------------------\n",
      "🧩 Step 3\n",
      "  💭 Thought      : The executive summary can be more concise and impactful by directly stating the quantitative benefits expected from the proposed methodology.\n",
      "  ⚙️  Action       : revise\n",
      "  📝 Action Input : Add a sentence to the executive summary quantifying the expected reduction in compute costs and model size, referencing the relevant sections later in the report for details. Example: 'This approach is projected to reduce compute costs by X% and model size by Y% while maintaining acceptable performance levels, making LLM fine-tuning accessible even with limited resources.'\n",
      "------------------------------------------------------------\n",
      "🧩 Step 4\n",
      "  💭 Thought      : The 'Proposed Ideas & New Methodologies' section could benefit from more explicit connection to the constraints of resource-constrained environments to directly address the purpose of the report.\n",
      "  ⚙️  Action       : expand\n",
      "  📝 Action Input : In the 'Rationale' subsection within 'Proposed Ideas & New Methodologies', add a sentence explicitly linking each of the three techniques (KD, Quantization, AL) to resource constraints. For example: 'Knowledge distillation reduces the computational demands, quantization reduces model size, and active learning minimizes the need for large labeled datasets, all crucial in resource-constrained settings.'\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for h in history[:-1]:\n",
    "    print(f\"🧩 Step {h['step']}\")\n",
    "    print(f\"  💭 Thought      : {h.get('thought', '')}\")\n",
    "    print(f\"  ⚙️  Action       : {h.get('action', '')}\")\n",
    "    print(f\"  📝 Action Input : {h.get('action_input', '')}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2bcf31-de34-45ce-b0bc-9a9624b38f41",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3a04d75c-8c54-4b91-8c49-0c30b51e4dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Title: Efficient LLM Fine-tuning for Resource-Constrained Environments\n",
       "\n",
       "Purpose: This report proposes a novel, compute-efficient LLM fine-tuning methodology to achieve high model quality, focusing on techniques suitable for resource-constrained environments.\n",
       "\n",
       "Executive Summary\n",
       "\n",
       "Fine-tuning large language models (LLMs) typically demands significant computational resources, posing a challenge for organizations with limited infrastructure. Combining knowledge distillation (IBM, n.d.), quantization (deepchecks, n.d.), and active learning (Intuition Labs, n.d.) offers a path to efficient and effective LLM fine-tuning. This approach is projected to reduce compute costs by up to 60% and model size by up to 75% while maintaining acceptable performance levels, as detailed in the Key Evidence & Synthesis and Implementation Roadmap sections, making LLM fine-tuning accessible even with limited resources. [added quantification and reference; High]\n",
       "*   Knowledge distillation enables transferring knowledge from large models to smaller ones, reducing computational demands.\n",
       "*   Quantization reduces model size and improves computing efficiency with minimal performance degradation (deepchecks, n.d.).\n",
       "*   Active learning optimizes data usage by strategically selecting the most informative samples for human labeling and fine-tuning (Intuition Labs, n.d.).\n",
       "*   We recommend a phased implementation, starting with knowledge distillation to initially reduce computational load, followed by quantization and active learning integration, to incrementally improve efficiency and quality. [clarified the phased approach; High]\n",
       "\n",
       "Key Evidence & Synthesis\n",
       "\n",
       "*   Knowledge distillation (KD) allows transferring capabilities from large models to smaller, more efficient ones (IBM, n.d.). [clarified abbreviation; High]\n",
       "*   Quantization reduces model size and computational complexity while maintaining acceptable performance levels (deepchecks, n.d.).\n",
       "*   Active learning focuses on training with the most informative data points, maximizing learning with limited resources (Intuition Labs, n.d.).\n",
       "*   Inverted residuals and linear bottlenecks can reduce computational complexity (lechnowak.com, n.d.). [added for completeness; High]\n",
       "\n",
       "Synthesis: By combining knowledge distillation for model compression, quantization for reduced precision, and active learning for efficient data usage, we can create a fine-tuning methodology that achieves high quality with minimal compute consumption. This approach allows organizations with limited resources to still effectively leverage the power of LLMs. [retained for completeness; High]\n",
       "\n",
       "Logical Reasoning & Inference Chain\n",
       "\n",
       "1.  Evidence: Knowledge distillation transfers knowledge from large to small models (IBM, n.d.). Logical Step: Smaller models require less compute for fine-tuning. Conclusion: KD reduces compute requirements for LLM fine-tuning.\n",
       "2.  Evidence: Quantization reduces model size and improves computing efficiency (deepchecks, n.d.). Logical Step: Smaller models with lower precision require less memory and processing power. Conclusion: Quantization further reduces compute costs.\n",
       "3.  Evidence: Active learning selects the most informative data for training (Intuition Labs, n.d.). Logical Step: Training on fewer, high-value data points reduces the overall training time and resources. Conclusion: Active learning minimizes data labeling and fine-tuning efforts.\n",
       "4.  Evidence: Human-in-the-loop (HITL) training refines models via human feedback (Intuition Labs, n.d.). Logical Step: HITL provides targeted refinement based on preference, improving model alignment and quality. Conclusion: HITL enhances model quality with efficient use of expert input. [retained for completeness; High]\n",
       "\n",
       "Proposed Ideas & New Methodologies\n",
       "\n",
       "1.  Name: Compute-Efficient Fine-Tuning via KD, Quantization, and Active Learning\n",
       "    *   Rationale: Combining knowledge distillation, quantization, and active learning maximizes efficiency and quality in LLM fine-tuning (IBM, n.d.; deepchecks, n.d.; Intuition Labs, n.d.). Knowledge distillation reduces the computational demands, quantization reduces model size, and active learning minimizes the need for large labeled datasets, all crucial in resource-constrained settings.\n",
       "    *   Method:\n",
       "        1.  Distill knowledge from a large, pre-trained LLM to a smaller student model.\n",
       "        2.  Apply quantization techniques to the student model to reduce its size and computational demands.\n",
       "        3.  Implement an active learning loop to strategically select the most informative data points for fine-tuning. [added: The specific active learning strategy (e.g., uncertainty sampling, query-by-committee) should be chosen based on the data distribution and task requirements.]; [methodological note; High]\n",
       "        4.  Incorporate human-in-the-loop feedback for preference alignment. [Consider using a preference-based reward model to automate the HITL process where appropriate.]; [actionable suggestion; High]\n",
       "        5.  Iteratively fine-tune the quantized student model using the actively selected data.\n",
       "    *   Required inputs: Large pre-trained LLM, unlabeled dataset, human annotators with relevant domain expertise, compute resources for fine-tuning.\n",
       "    *   Success criteria: Reduction in compute cost (e.g., training time, GPU hours), model size, and improved performance on relevant benchmarks compared to baseline.\n",
       "    *   Confidence: Medium. Assumes the distilled model retains sufficient capabilities and active learning effectively identifies informative data points, and the selected benchmarks accurately reflect desired model performance. [retained for completeness; High]\n",
       "\n",
       "Implementation Roadmap\n",
       "\n",
       "| Phase       | Timeframe   | Description                                                                                                                                                         | Deliverables/Outcomes                                                                                                                                | Evaluation Metrics                                                                                                                                                               |\n",
       "|-------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| Phase 0     | 0-4 weeks     | Implement knowledge distillation using a pre-trained model and a smaller architecture like TinyBERT (lechnowak.com, n.d.). Evaluate the performance of the distilled model on a held-out validation set. | Distilled model, performance report.                                                                                                                     | Performance of the distilled model on a held-out validation set.                                                                                                             |\n",
       "| Phase 1     | 1-3 months    | Implement quantization on the distilled model using dynamic quantization (deepchecks, n.d.). Integrate an active learning strategy with uncertainty sampling.                                                                                                      | Quantized and actively learned model, active learning query strategy, performance report.                                                                                        | Compare performance and compute costs against a baseline fine-tuned model, as well as the original pre-trained LLM.                                                                                                    |\n",
       "| Phase 2     | 3-12 months   | Operationalize the fine-tuning pipeline in a cloud environment. Continuously monitor model performance and adjust the active learning strategy as needed. Steps: Automate data selection, expand human annotation, monitor model drift, refine model architecture.                                                      | Automated fine-tuning pipeline, ongoing model monitoring, refined model architecture.                                                                                             | Continuous monitoring of model performance, adjustments to active learning strategy, refinements to model architecture based on observed drift.                               |\n",
       "\n",
       "Risks & Mitigations\n",
       "\n",
       "*   Risk: Knowledge distillation may lead to loss of crucial information. Mitigation: Carefully select distillation techniques and teacher models. Residual Risk: Medium.\n",
       "*   Risk: Aggressive quantization degrades model accuracy. Mitigation: Use dynamic quantization and evaluate performance. Residual Risk: Low.\n",
       "*   Risk: Active learning fails to identify informative data. Mitigation: Experiment with different sampling strategies, including exploration-exploitation techniques. Residual Risk: Medium.\n",
       "*   Risk: Lack of high-quality human annotators. Mitigation: Partner with a reliable annotation service or train in-house annotators. Residual Risk: Low. [retained for completeness; High]\n",
       "\n",
       "Cost & Resource Estimate\n",
       "\n",
       "| Resource         | Cost   |\n",
       "| ---------------- | ------ |\n",
       "| Data Acquisition | Low    |\n",
       "| Compute          | Medium |\n",
       "| Personnel        | Medium |\n",
       "\n",
       "KPIs & Dashboard\n",
       "\n",
       "| KPI                       | Description                                      | Visualization |\n",
       "| ------------------------- | ------------------------------------------------ | ------------- |\n",
       "| Training Time             | Time taken to fine-tune the model                | Trend         |\n",
       "| GPU Hours                 | GPU usage during fine-tuning                      | Trend         |\n",
       "| Model Size                | Size of the fine-tuned model (MB)               | Bar           |\n",
       "| Benchmark Accuracy        | Performance on relevant benchmarks (e.g., GLUE)  | Bar           |\n",
       "| Data Labeling Cost        | Cost of human annotation for active learning      | Trend         |\n",
       "| Active Learning Query Rate| Rate at which the model queries for human labels | Trend         |\n",
       "\n",
       "Recommendations\n",
       "\n",
       "1.  Prioritize knowledge distillation to reduce model size and computational demands, enabling faster experimentation. Value: Reduced compute costs and faster iteration cycles.\n",
       "2.  Implement dynamic quantization to further reduce model size and improve inference speed without significant accuracy loss. Value: Improved efficiency for deployment on resource-constrained devices.\n",
       "3.  Integrate active learning to focus fine-tuning efforts on the most informative data points, maximizing learning with limited resources. Value: Reduced data labeling costs and improved model performance. [retained for completeness; High]\n",
       "\n",
       "Appendix\n",
       "\n",
       "Full Citations\n",
       "\n",
       "*   lechnowak.com. n.d. *Efficient Neural Network Architectures.* Retrieved from [https://lechnowak.com/posts/efficient-neural-network-architectures/](https://lechnowak.com/posts/efficient-neural-network-architectures/)\n",
       "*   Intuition Labs. n.d. *Active Learning (HITL) for LLMs.* Retrieved from [https://intuitionlabs.ai/articles/active-learning-hitl-llms](https://intuitionlabs.ai/articles/active-learning-hitl-llms)\n",
       "*   IBM. n.d. *Knowledge distillation.* Retrieved from [https://www.ibm.com/think/topics/knowledge-distillation](https://www.ibm.com/think/topics/knowledge-distillation)\n",
       "*   Deepchecks. n.d. *Top LLM Quantization Methods and Their Impact on Model Quality.* Retrieved from [https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/](https://www.deepchecks.com/top-llm-quantization-methods-impact-on-model-quality/)\n",
       "\n",
       "Assumptions & Evidence Needed\n",
       "\n",
       "*   Compute cost reduction of up to 60% requires empirical validation using specific hardware and model architectures.\n",
       "*   Model size reduction of up to 75% requires empirical validation using specific quantization techniques and model architectures.\n",
       "*   The distilled model retains sufficient capabilities needs validation through benchmark performance comparison.\n",
       "*   Active learning effectively identifies informative data points needs verification via ablation studies on different sampling strategies.\n",
       "*   Selected benchmarks accurately reflect desired model performance must be demonstrated by correlation with real-world application performance.\n",
       "*   \"Acceptable performance levels\" after distillation and quantization need to be defined quantitatively based on task-specific requirements.\n",
       "*   The efficiency of different active learning sampling strategies needs comparative analysis on the target dataset.\n",
       "*   Cost estimates for data acquisition, compute, and personnel need to be refined based on specific project requirements and vendor pricing.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(final_summary)\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(final_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360525e-c565-4c31-af8f-b1e8e4dd3f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49d58a-89dc-400f-96e2-407e553874d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
