{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62585761-4711-4dbf-9c61-1281bbe9ebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Initialization Error: GEMINI_API_KEY and TAVILY_API_KEY environment variables must be set. Please run 'os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY\"' before execution.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid reducer signature. Expected (a, b) -> c. Got (*args, **kwargs)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 241\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m app\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Create the graph instance\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m app \u001b[38;5;241m=\u001b[39m create_graph()\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# 5. Execute the Agent\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Define the user's specific problem query\u001b[39;00m\n\u001b[0;32m    248\u001b[0m user_query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a competitive analysis report for clothing stores in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKoramangala 5th Block area, Bangalore. I need data on at least \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFocus on recent trends from 2024.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 214\u001b[0m, in \u001b[0;36mcreate_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_graph\u001b[39m():\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Builds and compiles the LangGraph state machine.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     workflow \u001b[38;5;241m=\u001b[39m StateGraph(AgentState)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# 1. Add Nodes\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     workflow\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, call_model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:243\u001b[0m, in \u001b[0;36mStateGraph.__init__\u001b[1;34m(self, state_schema, context_schema, input_schema, output_schema, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_schema \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m[OutputT], output_schema \u001b[38;5;129;01mor\u001b[39;00m state_schema)\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_schema \u001b[38;5;241m=\u001b[39m context_schema\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_schema)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:256\u001b[0m, in \u001b[0;36mStateGraph._add_schema\u001b[1;34m(self, schema, allow_managed)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas:\n\u001b[0;32m    255\u001b[0m     _warn_invalid_state_schema(schema)\n\u001b[1;32m--> 256\u001b[0m     channels, managed, type_hints \u001b[38;5;241m=\u001b[39m _get_channels(schema)\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_managed:\n\u001b[0;32m    258\u001b[0m         names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(managed)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:1320\u001b[0m, in \u001b[0;36m_get_channels\u001b[1;34m(schema)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   1313\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m: _get_channel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)},\n\u001b[0;32m   1314\u001b[0m         {},\n\u001b[0;32m   1315\u001b[0m         {},\n\u001b[0;32m   1316\u001b[0m     )\n\u001b[0;32m   1318\u001b[0m type_hints \u001b[38;5;241m=\u001b[39m get_type_hints(schema, include_extras\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1319\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1320\u001b[0m     name: _get_channel(name, typ)\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, typ \u001b[38;5;129;01min\u001b[39;00m type_hints\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__slots__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1323\u001b[0m }\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   1325\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_keys\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, BaseChannel)},\n\u001b[0;32m   1326\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_keys\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m is_managed_value(v)},\n\u001b[0;32m   1327\u001b[0m     type_hints,\n\u001b[0;32m   1328\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:1360\u001b[0m, in \u001b[0;36m_get_channel\u001b[1;34m(name, annotation, allow_managed)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     channel\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m channel\n\u001b[1;32m-> 1360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m channel \u001b[38;5;241m:=\u001b[39m _is_field_binop(annotation):\n\u001b[0;32m   1361\u001b[0m     channel\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m channel\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:1398\u001b[0m, in \u001b[0;36m_is_field_binop\u001b[1;34m(typ)\u001b[0m\n\u001b[0;32m   1396\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m BinaryOperatorAggregate(typ, meta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1398\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1399\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reducer signature. Expected (a, b) -> c. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1400\u001b[0m             )\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid reducer signature. Expected (a, b) -> c. Got (*args, **kwargs)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List, Literal\n",
    "from operator import itemgetter\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from langgraph.graph import StateGraph, END\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# We ONLY need StateGraph, END, and basic Python types.\n",
    "# We explicitly AVOID: from langgraph.prebuilt import ToolExecutor\n",
    "# We explicitly AVOID: from langgraph.prebuilt import ToolNode \n",
    "# (We will use a standard function for the tool execution node instead)\n",
    "\n",
    "# --- Configuration & Initialization ---\n",
    "try:\n",
    "    if not os.getenv(\"GEMINI_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
    "        raise EnvironmentError(\n",
    "            \"GEMINI_API_KEY and TAVILY_API_KEY environment variables must be set. \"\n",
    "            \"Please run 'os.environ[\\\"GEMINI_API_KEY\\\"] = \\\"YOUR_KEY\\\"' before execution.\"\n",
    "        )\n",
    "    google_api_key = \"API_KEY\"\n",
    "    tavily_api_key = \"API_KEY\"\n",
    "    \n",
    "    os.environ[\"GEMINI_API_KEY\"] = google_api_key\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "    \n",
    "    # --- Initialize the Direct Google GenAI Client ---\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    # Initialize Clients\n",
    "    client = genai.Client()\n",
    "    tavily_client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
    "\n",
    "    print(\"‚úÖ Clients initialized successfully.\")\n",
    "    print(\"Model: Gemini-2.0-Flash | Search Tool: Tavily | Framework: LangGraph (Custom Tool Node)\")\n",
    "\n",
    "except EnvironmentError as e:\n",
    "    print(f\"‚ùå Initialization Error: {e}\")\n",
    "    # Halt execution if environment is not set up correctly\n",
    "    client = None \n",
    "    tavily_client = None\n",
    "    exit() \n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Define Tool and State\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Dictionary mapping tool names (as seen by the LLM) to the actual Python function\n",
    "AVAILABLE_TOOLS = {\n",
    "    \"tavily_search\": lambda query: tavily_search(query)\n",
    "}\n",
    "\n",
    "def tavily_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a real-time web search using the Tavily API to find competitor data \n",
    "    including store footfall and busiest times.\n",
    "    \"\"\"\n",
    "    if tavily_client is None: return \"Search client not initialized.\"\n",
    "        \n",
    "    print(f\"\\n--- üîé Searching Tavily for: '{query}' ---\")\n",
    "    try:\n",
    "        response = tavily_client.search(\n",
    "            query=query, \n",
    "            search_depth=\"advanced\", \n",
    "            max_results=5\n",
    "        )\n",
    "        results_summary = []\n",
    "        for result in response.get('results', []):\n",
    "            results_summary.append(f\"Source: {result['url']}\\nSnippet: {result['content']}\")\n",
    "            \n",
    "        if not results_summary: return \"No relevant search results found for the query.\"\n",
    "            \n",
    "        return \"Search Results:\\n\" + \"\\n\\n\".join(results_summary)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during the search: {e}\"\n",
    "\n",
    "# Graph State Definition\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Represents the state of our graph.\"\"\"\n",
    "    input: str\n",
    "    chat_history: Annotated[List[types.Content], itemgetter(\"chat_history\")] \n",
    "    report: str\n",
    "    # 'tool_calls' holds the function calls requested by the model.\n",
    "    tool_calls: Annotated[List[types.Part], itemgetter(\"tool_calls\")] \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.setdefault('chat_history', [])\n",
    "        self.setdefault('report', \"\")\n",
    "        self.setdefault('tool_calls', [])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Define Graph Nodes (Functions)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def call_model(state: AgentState) -> dict:\n",
    "    \"\"\"The main agent node. Decides to call the tool or generate the final report.\"\"\"\n",
    "    print(\"--- üß† Calling Gemini Model ---\")\n",
    "    \n",
    "    system_instruction = (\n",
    "        \"You are an expert Competitive Market Analyst AI for clothing stores. \"\n",
    "        \"Your task is to generate a comprehensive 'Competitor Analysis Report' \"\n",
    "        \"based on the user's request and the information you gather using the 'tavily_search' tool. \"\n",
    "        \"You MUST use the tool to find real-time data on competitors, footfall, and busiest times. \"\n",
    "        \"Only generate the final report AFTER you have all the necessary search results. \"\n",
    "        \"Your final output MUST be a structured report with sections: \"\n",
    "        \"1. Competitor Identification, 2. Footfall & Peak Hour Analysis, 3. Strategic Recommendations.\"\n",
    "    )\n",
    "    \n",
    "    # Prepare contents for the model call\n",
    "    user_content = types.Content(\n",
    "        role=\"user\", \n",
    "        parts=[types.Part.from_text(state[\"input\"])]\n",
    "    )\n",
    "    # The initial 'input' is treated as the first user message. Subsequent runs use history.\n",
    "    \n",
    "    # In LangGraph, when looping, we only pass the history, and the new input is \n",
    "    # considered part of the current step's state update.\n",
    "    # However, to maintain multi-turn, we must append the current input to history for the call.\n",
    "    messages = state[\"chat_history\"] + [user_content]\n",
    "    \n",
    "    # If this is a loop from tool execution, the previous model call (with tool_calls) \n",
    "    # and the subsequent tool results (from execute_tools) will be in 'chat_history'.\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=messages,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            tools=[tavily_search] # Pass the search tool definition\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 1. Update history with the model's response (tool call OR final text)\n",
    "    model_response_content = types.Content(role=\"model\", parts=response.parts)\n",
    "    new_chat_history = state[\"chat_history\"] + [user_content, model_response_content]\n",
    "\n",
    "    # 2. Check for function calls\n",
    "    if response.function_calls:\n",
    "        # Model decided to use a tool, return tool calls for the next node\n",
    "        return {\"tool_calls\": response.function_calls, \"chat_history\": new_chat_history}\n",
    "\n",
    "    # 3. Model returned a final text response (the report)\n",
    "    return {\"report\": response.text, \"chat_history\": new_chat_history}\n",
    "\n",
    "\n",
    "def execute_tools(state: AgentState) -> dict:\n",
    "    \"\"\"Custom node to execute the tools requested by the model.\"\"\"\n",
    "    print(\"--- üõ†Ô∏è Executing Custom Tool Node ---\")\n",
    "    \n",
    "    tool_calls = state.get(\"tool_calls\", [])\n",
    "    tool_results = []\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        func_name = tool_call.function.name\n",
    "        func_args = dict(tool_call.function.args) # Convert to dictionary for easy access\n",
    "        \n",
    "        # Get the Python function and execute it\n",
    "        if func_name in AVAILABLE_TOOLS:\n",
    "            tool_function = AVAILABLE_TOOLS[func_name]\n",
    "            # Tavily search only needs the query argument\n",
    "            # We assume the model correctly fills the 'query' argument\n",
    "            result_content = tool_function(**func_args)\n",
    "            \n",
    "            # Format the result back into the GenAI SDK's expected structure\n",
    "            tool_results.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=func_name,\n",
    "                    response={\"content\": result_content}\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            result_content = f\"Error: Tool '{func_name}' not found.\"\n",
    "            tool_results.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=func_name,\n",
    "                    response={\"content\": result_content}\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    # Add tool results to the chat history as content with role=\"tool\"\n",
    "    # We must find the last message (which contains the tool call request) \n",
    "    # and append the tool results after it.\n",
    "    \n",
    "    # In this custom implementation, the tool call request (from the model) is already in history.\n",
    "    # We just need to append the tool results as a separate message.\n",
    "    tool_content = types.Content(role=\"tool\", parts=tool_results)\n",
    "    new_chat_history = state[\"chat_history\"] + [tool_content]\n",
    "    \n",
    "    # Clear tool_calls to signal that execution is complete\n",
    "    return {\"chat_history\": new_chat_history, \"tool_calls\": []}\n",
    "\n",
    "\n",
    "def route_next_step(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "    \"\"\"Conditional edge logic: decide whether to call the tool or end the graph.\"\"\"\n",
    "    # We check the tool_calls returned by the last 'agent' node run.\n",
    "    if state.get(\"tool_calls\"):\n",
    "        print(\"--- ‚û°Ô∏è Route: Execute Tools ---\")\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        print(\"--- üõë Route: Final Report & End ---\")\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Build and Compile the LangGraph\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def create_graph():\n",
    "    \"\"\"Builds and compiles the LangGraph state machine.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # 1. Add Nodes\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    workflow.add_node(\"tools\", execute_tools) # Custom tool execution function\n",
    "\n",
    "    # 2. Set Start\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    \n",
    "    # 3. Define Edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        route_next_step,\n",
    "        {\n",
    "            \"tools\": \"tools\", # If tool is called, execute the custom tool node\n",
    "            \"end\": END,       # If report is ready, stop\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # After custom tool execution, loop back to the agent for the next step of reasoning\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    \n",
    "    app = workflow.compile()\n",
    "    print(\"‚úÖ LangGraph compiled successfully!\")\n",
    "    return app\n",
    "\n",
    "# Create the graph instance\n",
    "app = create_graph()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5. Execute the Agent\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Define the user's specific problem query\n",
    "user_query = (\n",
    "    \"Generate a competitive analysis report for clothing stores in the \"\n",
    "    \"Koramangala 5th Block area, Bangalore. I need data on at least \"\n",
    "    \"three major competitors, their peak customer footfall hours, and \"\n",
    "    \"strategic recommendations to maximize engagement against them. \"\n",
    "    \"Focus on recent trends from 2024.\"\n",
    ")\n",
    "\n",
    "initial_state = {\"input\": user_query, \"chat_history\": []}\n",
    "\n",
    "print(f\"\\n==================================================================\")\n",
    "print(f\"| Starting Agent Run with Query:                                   |\")\n",
    "print(f\"| {user_query} |\")\n",
    "print(f\"==================================================================\\n\")\n",
    "\n",
    "# Run the graph\n",
    "try:\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    # 6. Display the Report\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL COMPETITOR ANALYSIS REPORT üìä\")\n",
    "    print(\"=\"*80)\n",
    "    print(final_state['report'])\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ‚ùå Execution Failed ---\")\n",
    "    print(f\"An error occurred during the LangGraph execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99ded1-afd7-41d8-9750-7790daf2b77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
