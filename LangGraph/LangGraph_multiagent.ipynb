{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62562da-ee38-43fd-bbf9-f670f6b854c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c721cc2-afd3-44bf-b26b-17d0aea9519b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3952962-d06b-41af-9591-a39924820610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638e3286-86da-4545-9d6a-89cd4d6014cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from typing import Dict, Any, List, Optional\n",
    "from typing import TypedDict, Literal\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent  # agent builder used in ref\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Tavily search (community tool)\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# LangGraph imports (StateGraph-based flow)\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Chat interface used in your reference\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10fa4249-086d-45d6-b0ca-dd002350973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"FinancialAssistant/1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f5363b6-5ff7-4cbc-8133-16bcb125a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Router keywords\n",
    "WEB_KEYWORDS = [r\"\\blatest\\b\", r\"\\btoday\\b\", r\"\\bcurrent\\b\", r\"\\brecent\\b\", r\"\\bbreaking\\b\"]\n",
    "RAG_KEYWORDS = [r\"\\bpolicy\\b\", r\"\\binternal\\b\", r\"\\bproduct\\b\", r\"\\bmanual\\b\", r\"\\bknowledge base\\b\"]\n",
    "\n",
    "# LLM factory (ChatOpenAI wrapper used in your snippet)\n",
    "def get_chat_llm(model: str = \"gpt-4o-mini\", temperature: float = 0.0) -> ChatOpenAI:\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set in environment\")\n",
    "    return ChatOpenAI(model=model, temperature=temperature, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Lightweight HTML text extractor\n",
    "def extract_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # remove script/style\n",
    "    for s in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        s.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    text = \"\\n\".join([l for l in lines if l])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a87b41-fc49-4b0c-bb18-e3d4e961f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def tavily_search(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Use TavilySearchResults to find tourist guide/blog pages for `city`,\n",
    "    fetch pages, and summarize them with the ChatOpenAI model.\n",
    "    Returns a plain text aggregation of summaries.\n",
    "    \"\"\"\n",
    "    if not TAVILY_API_KEY:\n",
    "        return \"TAVILY_API_KEY not set; cannot run Tavily search.\"\n",
    "    # Build search query\n",
    "    query = f\"Top tourist attractions in {city} travel guide blog\"\n",
    "    # Use the Tavily tool wrapper\n",
    "    search = TavilySearchResults(k=6, api_key=TAVILY_API_KEY)\n",
    "    results = search.run(query)\n",
    "    urls = [r.get(\"url\") for r in results if \"url\" in r][:5]\n",
    "\n",
    "    llm = get_chat_llm(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    summaries: List[str] = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            text = extract_text(resp.text)\n",
    "            # trim to reasonable length for LLM\n",
    "            text = text[:12000]\n",
    "\n",
    "            prompt = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a travel content summarizer. Produce a crisp summary.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following webpage content for travel insights:\\n\\n{text}\"}\n",
    "            ]\n",
    "            response = llm.invoke(prompt)\n",
    "            summary = getattr(response, \"content\", str(response))\n",
    "            summaries.append(f\"URL: {url}\\nSummary: {summary}\\n\")\n",
    "        except Exception as e:\n",
    "            summaries.append(f\"URL: {url}\\nError summarizing page: {e}\\n\")\n",
    "\n",
    "    return \"\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e578a6-97a3-41bd-85ee-77ca26b7e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def rag_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG retrieval tool. Expects a FAISS vectorstore saved at ./faiss_store or built at runtime.\n",
    "    Returns top-k documents joined as a text blob.\n",
    "    \"\"\"\n",
    "    # Use OpenAIEmbeddings via LangChain if OPENAI_API_KEY present\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"OPENAI_API_KEY not set; cannot run RAG retrieval.\"\n",
    "\n",
    "    # We'll look for a persisted FAISS index in ./faiss_index or build a small in-memory store if not present.\n",
    "    # For demo, we will build an ephemeral index from sample docs if no persisted store exists.\n",
    "    sample_docs = [\n",
    "        \"Internal refund policy: customers can return items within 30 days given proof of purchase.\",\n",
    "        \"Product X spec: battery lasts up to 14 days under normal usage and weighs 1.2kg.\",\n",
    "        \"Company architecture: we use microservices and event-driven transport for async tasks.\"\n",
    "    ]\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "        # If you had persisted store: load it (not implemented here) else build ephemeral\n",
    "        vectorstore = FAISS.from_texts(sample_docs, embeddings)\n",
    "        docs_and_scores = vectorstore.similarity_search_with_score(query, k=5)\n",
    "        lines = []\n",
    "        for i, (doc, score) in enumerate(docs_and_scores):\n",
    "            lines.append(f\"Doc{i+1} (score={score}): {doc.page_content}\")\n",
    "        return \"\\n\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"Error in rag_search: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "950f365d-9013-46b7-a677-be94772574d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Router node: performs a quick heuristic check, then falls back to LLM.\n",
    "    Output: {\"route\": \"web_search\" | \"rag\" | \"llm\"}\n",
    "    \"\"\"\n",
    "    llm = get_chat_llm(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Classify the user query into one routing category. \"\n",
    "                \"Reply with exactly ONE of: web_search, rag, llm. \"\n",
    "                \"No explanation.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        resp = llm.invoke(prompt)\n",
    "        output = getattr(resp, \"content\", \"\").strip().lower()\n",
    "\n",
    "        # sanitize LLM output\n",
    "        if \"web_search\" in output:\n",
    "            return {\"route\": \"web_search\"}\n",
    "        if \"rag\" in output:\n",
    "            return {\"route\": \"rag\"}\n",
    "        if \"llm\" in output:\n",
    "            return {\"route\": \"llm\"}\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\"route\": \"llm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "928d63f2-2845-4a0f-9f2c-3e6e4190f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Synthesize final answer from evidence present in the state.\n",
    "    Expects state to have:\n",
    "      - city (optional)\n",
    "      - route\n",
    "      - evidence (string or list of source strings)\n",
    "      - weather (optional)\n",
    "    Returns {\"final_report\": \"<text>\"}\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    city = state.get(\"city\", \"the city\")\n",
    "    route = state.get(\"route\", \"llm\")\n",
    "    evidence = state.get(\"evidence\", \"\")\n",
    "    weather = state.get(\"weather\", \"\")\n",
    "\n",
    "    # normalize evidence into single text blob\n",
    "    if isinstance(evidence, list):\n",
    "        ev_text = \"\\n\\n\".join(evidence)\n",
    "    else:\n",
    "        ev_text = str(evidence)\n",
    "\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert travel report writer. Produce executive-level summaries.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"\"\"\n",
    "Write an executive blog-style travel report for **{city}** using the evidence and current weather below.\n",
    "\n",
    "--- CURRENT WEATHER ---\n",
    "{weather}\n",
    "\n",
    "--- EVIDENCE (from {route}) ---\n",
    "{ev_text}\n",
    "\n",
    "Make sure the final report explicitly includes:\n",
    "- Current weather condition (sunny, cloudy, rainy, etc.)\n",
    "- Temperature and how it feels (if available)\n",
    "- Humidity levels (if available)\n",
    "- Wind conditions (if available)\n",
    "- How the weather affects travel experience\n",
    "- Whether the weather is ideal for visiting right now\n",
    "\n",
    "Output format:\n",
    "**Executive Summary**\n",
    "**Current Weather Conditions**\n",
    "**Key Insights**\n",
    "**Best Time to Visit**\n",
    "**Top Attractions**\n",
    "**Final Recommendation**\n",
    "\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    final = getattr(response, \"content\", str(response))\n",
    "    return {\"final_report\": final}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3f3361f-dfc7-4ccd-9211-8fa3a923d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. FLOW STATE\n",
    "# ------------------------------------------------------------\n",
    "class FlowState(TypedDict, total=False):\n",
    "    city: str\n",
    "    query: str\n",
    "\n",
    "    weather: str\n",
    "    tourist: str\n",
    "    evidence: str\n",
    "\n",
    "    route: Literal[\"web_search\", \"rag\", \"llm\"]\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. LLM FACTORY\n",
    "# ------------------------------------------------------------\n",
    "def get_llm(model=\"gpt-4o-mini\", temperature=0):\n",
    "    return ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. ACTUAL AGENT NODE\n",
    "# ------------------------------------------------------------\n",
    "def agent_node(state: FlowState) -> FlowState:\n",
    "    \"\"\"\n",
    "    Calls the Agent (weather + Tavily search).\n",
    "    The Agent must return JSON with keys weather + tourist.\n",
    "    \"\"\"\n",
    "\n",
    "    city = state.get(\"city\")\n",
    "    if not city:\n",
    "        raise ValueError(\"agent_node requires 'city' in FlowState\")\n",
    "\n",
    "    instruction = f\"\"\"\n",
    "    You are a research agent.\n",
    "    Return EXACT JSON with keys:\n",
    "    - \"weather\": a short description of weather for {city}.\n",
    "    - \"tourist\": a bullet list of attractions.\n",
    "\n",
    "    Example:\n",
    "    {{\n",
    "        \"weather\": \"...\",\n",
    "        \"tourist\": \"...\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    llm = get_llm()\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": f\"Research city {city}. Return ONLY JSON.\"}\n",
    "    ])\n",
    "\n",
    "    raw = resp.content\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        # fallback – extract JSON substring\n",
    "        try:\n",
    "            start = raw.index(\"{\")\n",
    "            end = raw.rindex(\"}\") + 1\n",
    "            data = json.loads(raw[start:end])\n",
    "        except:\n",
    "            data = {\"weather\": raw, \"tourist\": \"No tourist info extracted.\"}\n",
    "\n",
    "    return {\n",
    "        \"weather\": data.get(\"weather\", \"\"),\n",
    "        \"tourist\": data.get(\"tourist\", \"\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f7506a-8697-4ef9-a434-0d46483dcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. ROUTER NODE\n",
    "# ------------------------------------------------------------\n",
    "def router_node(state: FlowState) -> FlowState:\n",
    "    \"\"\"\n",
    "    Chooses route based on query.\n",
    "    Must output FlowState with 'route'.\n",
    "    \"\"\"\n",
    "\n",
    "    query = state.get(\"query\", \"\")\n",
    "    if not query:\n",
    "        return {\"route\": \"llm\"}\n",
    "\n",
    "    llm = get_llm()\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Classify the query into EXACTLY one of: \"\n",
    "                \"web_search, rag, llm. \"\n",
    "                \"Return ONLY one token.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ])\n",
    "\n",
    "    route = resp.content.strip().lower()\n",
    "\n",
    "    allowed = {\"web_search\", \"rag\", \"llm\"}\n",
    "    if route not in allowed:\n",
    "        route = \"llm\"\n",
    "\n",
    "    return {\"route\": route}\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. WEB SEARCH NODE (Tavily)\n",
    "# ------------------------------------------------------------\n",
    "def web_search_node(state: FlowState) -> FlowState:\n",
    "    q = state.get(\"query\") or state.get(\"city\") or \"\"\n",
    "    # Replace with TavilySearchResults run\n",
    "    evidence = f\"(Mock Tavily search results for: {q})\"\n",
    "    return {\"evidence\": evidence}\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. RAG NODE (FAISS / docstore)\n",
    "# ------------------------------------------------------------\n",
    "def rag_node(state: FlowState) -> FlowState:\n",
    "    q = state.get(\"query\", \"\")\n",
    "    # Replace with real RAG chain\n",
    "    evidence = f\"(Mock RAG retrieval for: {q})\"\n",
    "    return {\"evidence\": evidence}\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. LLM REASONING NODE\n",
    "# ------------------------------------------------------------\n",
    "def llm_node(state: FlowState) -> FlowState:\n",
    "    q = state.get(\"query\", \"\")\n",
    "    llm = get_llm()\n",
    "\n",
    "    resp = llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": \"Answer concisely.\"},\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ])\n",
    "\n",
    "    return {\"evidence\": resp.content}\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. FINAL SUMMARIZER NODE\n",
    "# ------------------------------------------------------------\n",
    "def summarizer_node(state: FlowState) -> FlowState:\n",
    "    weather = state.get(\"weather\", \"\")\n",
    "    tourist = state.get(\"tourist\", \"\")\n",
    "    evidence = state.get(\"evidence\", \"\")\n",
    "\n",
    "    llm = get_llm()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Produce a final summary:\n",
    "\n",
    "    WEATHER:\n",
    "    {weather}\n",
    "\n",
    "    ATTRACTIONS:\n",
    "    {tourist}\n",
    "\n",
    "    SUPPORTING EVIDENCE:\n",
    "    {evidence}\n",
    "    \"\"\"\n",
    "\n",
    "    resp = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    return {\"final_report\": resp.content}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b19e8b25-ccb5-4a71-83e5-31452eb102a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow():\n",
    "    graph = StateGraph(FlowState)\n",
    "\n",
    "    graph.add_node(\"agent\", agent_node)\n",
    "    graph.add_node(\"router\", router_node)\n",
    "\n",
    "    graph.add_node(\"web_search\", web_search_node)\n",
    "    graph.add_node(\"rag\", rag_node)\n",
    "    graph.add_node(\"llm_reason\", llm_node)\n",
    "\n",
    "    graph.add_node(\"summarizer\", summarizer_node)\n",
    "\n",
    "    graph.set_entry_point(\"agent\")\n",
    "\n",
    "    # agent → router\n",
    "    graph.add_edge(\"agent\", \"router\")\n",
    "\n",
    "    # router → conditional edges\n",
    "    graph.add_conditional_edges(\n",
    "        \"router\",\n",
    "        lambda s: s[\"route\"],\n",
    "        {\n",
    "            \"web_search\": \"web_search\",\n",
    "            \"rag\": \"rag\",\n",
    "            \"llm\": \"llm_reason\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # all routes → summarizer\n",
    "    graph.add_edge(\"web_search\", \"summarizer\")\n",
    "    graph.add_edge(\"rag\", \"summarizer\")\n",
    "    graph.add_edge(\"llm_reason\", \"summarizer\")\n",
    "\n",
    "    graph.add_edge(\"summarizer\", END)\n",
    "\n",
    "    return graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17dc5dc4-45ff-4bbc-8b3f-a919d37461cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_financial_flow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     flow \u001b[38;5;241m=\u001b[39m create_financial_flow()\n\u001b[0;32m      5\u001b[0m     input_state: FinancialAgentState \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [BaseMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the valuation and recent news for NVIDIA and Oracle.\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the valuation and recent news for NVIDIA and Oracle.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[0;32m     11\u001b[0m     }\n\u001b[0;32m     13\u001b[0m     result \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39minvoke(input_state)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_financial_flow' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    flow = create_flow()\n",
    "\n",
    "    input_state = {\n",
    "        \"city\": \"Chennai\",\n",
    "        \"query\": \"What is the best time to visit Chennai and the top attractions?\"\n",
    "    }\n",
    "\n",
    "    result = flow.invoke(input_state)\n",
    "\n",
    "    print(\"\\n===== FINAL REPORT =====\\n\")\n",
    "    print(result.get(\"final_report\") or result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c38849-d7d7-41bc-9cf3-b7a2e36a94f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde008c-fcb4-4632-9f80-0d0a2f9c3a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
