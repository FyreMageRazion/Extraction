{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cdb544-a472-41e5-91c9-b2174e9635ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with latest versions - langchain-1.0.2 langchain-core-1.0.0 langgraph-1.0.1 langgraph-prebuilt-1.0.1\n",
    "# https://docs.langchain.com/oss/python/migrate/langchain-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947e1b8-89a3-4e63-8fa7-3f77c86b3d04",
   "metadata": {},
   "source": [
    "# Refer to the code in the sessions and samples and try again -\n",
    "# using openAI itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33048e79-f5b2-49f7-8649-e61bbe1b2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from typing import TypedDict, Annotated, List, Literal, Optional, Any, Dict, Annotated\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import google.generativeai as genai\n",
    "from google.genai import types\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e31f427-1807-49bb-a692-2a523a7993c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "google_api_key = \"AIzaSyCJj_kjZGhiQ43ZvGb9AcRD7tzQdvvxFM8\"\n",
    "tavily_api_key = \"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\"\n",
    "    \n",
    "os.environ[\"GEMINI_API_KEY\"] = google_api_key\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api_key\n",
    "genai.configure(api_key=google_api_key)\n",
    "tavily_client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7fd2881-1d57-4e83-9f36-30b5a1ba7706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1416a3-9ed7-4c67-a7ff-36715da40c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to real-time information, including live weather updates. To find out the weather in Chennai today, I recommend checking a reliable weather app or website like:\n",
      "\n",
      "*   **Google Weather:** Simply search \"weather in Chennai\" on Google.\n",
      "*   **AccuWeather:** [https://www.accuweather.com/](https://www.accuweather.com/)\n",
      "*   **The Weather Channel:** [https://weather.com/](https://weather.com/)\n",
      "*   **India Meteorological Department (IMD):** [http://www.imd.gov.in/](http://www.imd.gov.in/)\n",
      "\n",
      "These sources will provide you with the most current and accurate weather information for Chennai.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "response = model.generate_content(\n",
    "    search_questions_system_prompt\n",
    ")\n",
    "\n",
    "text = response.candidates[0].content.parts[0].text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67089e-f2e8-47c3-a0a3-236469bdf3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "377ca066-d1fc-4ce0-b87b-7b9a860ea04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai.types import FunctionDeclaration, Tool\n",
    "from google.generativeai.client import get_default_retriever_client\n",
    "from google.generativeai import GenerativeModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Example tool function\n",
    "def get_current_weather(location: str):\n",
    "    \"\"\"\n",
    "    Gets the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The city or location to get the weather for.\n",
    "    Returns:\n",
    "        str: A string describing the weather.\n",
    "    \"\"\"\n",
    "    if location == \"chennai\":\n",
    "        return \"It's cloudy with a temperature of 15 degrees Celsius.\"\n",
    "    else:\n",
    "        return \"Weather data not available for this location.\"\n",
    "\n",
    "# Define the tool schema\n",
    "get_weather_tool = FunctionDeclaration(\n",
    "    name=\"get_current_weather\",\n",
    "    description=\"Gets the current weather for a specified location.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city or location\"}},\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create a Tool object from the FunctionDeclaration\n",
    "tools_list = [Tool(function_declarations=[get_weather_tool])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4b4777b-5f9d-43ac-a5d7-c26a1f09eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_questions_system_prompt = \"Hows the weather in chennai today? Utilize the available tools as needed for the response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f8a1300-dd58-4817-9572-6b430081a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash\", tools=tools_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95c78616-a5bd-4cb8-a0cc-541c0c2119c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Send a prompt that requires tool use\n",
    "response = model.generate_content(\n",
    "    search_questions_system_prompt\n",
    ")\n",
    "\n",
    "text = response.candidates[0].content.parts[0].text\n",
    "print(text)\n",
    "\n",
    "# # Process the response (e.g., call the tool if the model requests it)\n",
    "# if response.candidates[0].content.parts[0].function_call:\n",
    "#     function_call = response.candidates[0].content.parts[0].function_call\n",
    "#     if function_call.name == \"get_current_weather\":\n",
    "#         result = get_current_weather(**function_call.args)\n",
    "#         # Send the tool output back to the model for a final response\n",
    "#         response_with_tool_output = model.generate_content(\n",
    "#             [prompt, response.candidates[0].content, {\"tool_outputs\": [result]}]\n",
    "#         )\n",
    "#         print(response_with_tool_output.text)\n",
    "# else:\n",
    "#     print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba161ed0-e15a-4471-b627-3b8760800134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfb353-caf4-4bd0-80c8-111007b8d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a3d19-a4f1-4135-933f-09c9b7cc026a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26134815-9c22-45e9-a9c5-a2618fa433e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"get_current_weather\",\n",
       "                  \"args\": {\n",
       "                    \"location\": \"chennai\"\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -7.340922820731066e-06\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 40,\n",
       "        \"candidates_token_count\": 8,\n",
       "        \"total_token_count\": 48,\n",
       "        \"prompt_tokens_details\": [\n",
       "          {\n",
       "            \"modality\": \"TEXT\",\n",
       "            \"token_count\": 40\n",
       "          }\n",
       "        ],\n",
       "        \"candidates_tokens_details\": [\n",
       "          {\n",
       "            \"modality\": \"TEXT\",\n",
       "            \"token_count\": 8\n",
       "          }\n",
       "        ]\n",
       "      },\n",
       "      \"model_version\": \"gemini-2.0-flash\",\n",
       "      \"response_id\": \"kCL6aK-9Gf3Zvr0P4rubuAg\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43af854-6130-496a-ae6e-c5c4d9771863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# LangChain imports (newer LangChain has split modules; we try to import the canonical names)\n",
    "try:\n",
    "    # recommended integration packages\n",
    "    from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "    from langchain_core.tools import tool\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    from langchain_tavily import TavilySearch\n",
    "except Exception as e:\n",
    "    # Fallback imports and informative error\n",
    "    raise ImportError(\n",
    "        \"Could not import LangChain integration modules. \"\n",
    "        \"Make sure you installed `langchain`, `langchain-google-genai`, and `langchain-tavily`.\\n\"\n",
    "        f\"Underlying error: {e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2d6bc-290e-46ab-963f-caed62299553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81761760-3a32-4f80-bacb-7882c3cf64a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a856e-7995-41e6-8e29-4294edc958fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Environment variables\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.environ.get(\"TAVILY_API_KEY\")\n",
    "WEATHERAPI_KEY = os.environ.get(\"WEATHERAPI_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise EnvironmentError(\"Please set environment variable GOOGLE_API_KEY to your Gemini API key.\")\n",
    "if not TAVILY_API_KEY:\n",
    "    raise EnvironmentError(\"Please set environment variable TAVILY_API_KEY to your Tavily API key.\")\n",
    "if not WEATHERAPI_KEY:\n",
    "    raise EnvironmentError(\"Please set environment variable WEATHERAPI_KEY to your WeatherAPI.com API key.\")\n",
    "\n",
    "\n",
    "#\n",
    "# 1) Define the Weather Tool (uses WeatherAPI.com)\n",
    "#\n",
    "@tool\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns current weather for the given city using WeatherAPI.com.\n",
    "    Example response: \"Chennai: Partly cloudy, 29°C (feels like 31°C). Condition: Partly cloudy.\"\n",
    "    \"\"\"\n",
    "    if not city:\n",
    "        return \"No city provided.\"\n",
    "\n",
    "    # WeatherAPI current weather endpoint\n",
    "    base = \"http://api.weatherapi.com/v1/current.json\"\n",
    "    params = {\"key\": WEATHERAPI_KEY, \"q\": city, \"aqi\": \"no\"}\n",
    "\n",
    "    try:\n",
    "        r = requests.get(base, params=params, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        location = j.get(\"location\", {})\n",
    "        current = j.get(\"current\", {})\n",
    "        name = location.get(\"name\") or city\n",
    "        region = location.get(\"region\")\n",
    "        country = location.get(\"country\")\n",
    "        temp_c = current.get(\"temp_c\")\n",
    "        feelslike_c = current.get(\"feelslike_c\")\n",
    "        condition = current.get(\"condition\", {}).get(\"text\")\n",
    "        humidity = current.get(\"humidity\")\n",
    "        wind_kph = current.get(\"wind_kph\")\n",
    "        out = (\n",
    "            f\"{name}, {region or ''} {country or ''} — {condition}. \"\n",
    "            f\"Temperature: {temp_c}°C (feels like {feelslike_c}°C). \"\n",
    "            f\"Humidity: {humidity}%. Wind: {wind_kph} kph.\"\n",
    "        )\n",
    "        return out\n",
    "    except requests.RequestException as exc:\n",
    "        return f\"Weather API request failed: {exc}\"\n",
    "    except Exception as exc:\n",
    "        return f\"Unexpected error while fetching weather: {exc}\"\n",
    "\n",
    "\n",
    "#\n",
    "# 2) Instantiate TavilySearch wrapper and expose it as a LangChain tool.\n",
    "#    LangChain's create_tool_calling_agent expects a list of \"tools\" (callables decorated with @tool).\n",
    "#\n",
    "def make_tavily_search_tool():\n",
    "    \"\"\"\n",
    "    Create a LangChain-compatible tool wrapper around TavilySearch.\n",
    "    Returns a callable decorated with @tool (so it becomes a tool in create_tool_calling_agent).\n",
    "    \"\"\"\n",
    "    # instantiate TavilySearch with API key (langchain-tavily expects env or param)\n",
    "    # The langchain-tavily integration typically reads TAVILY_API_KEY from env;\n",
    "    # but we'll create the client explicitly if constructor allows.\n",
    "    tavily = TavilySearch(api_key=TAVILY_API_KEY)\n",
    "\n",
    "    # define an inner function and decorate it as a tool\n",
    "    @tool\n",
    "    def tavily_search(query: str, max_results: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Search the web via Tavily for the query and return a concise summary\n",
    "        of the top results (title + snippet), concatenated.\n",
    "        \"\"\"\n",
    "        if not query:\n",
    "            return \"No query provided.\"\n",
    "\n",
    "        try:\n",
    "            results = tavily.run(query, max_results=max_results)\n",
    "            # Results may be a list of dicts or a string depending on integration - normalize\n",
    "            if isinstance(results, str):\n",
    "                return results\n",
    "            if isinstance(results, (list, tuple)):\n",
    "                summaries = []\n",
    "                for idx, item in enumerate(results[:max_results], 1):\n",
    "                    # item may have 'title' and 'snippet' fields (normalize defensively)\n",
    "                    title = item.get(\"title\") or item.get(\"headline\") or item.get(\"name\") or \"\"\n",
    "                    snippet = item.get(\"snippet\") or item.get(\"summary\") or item.get(\"excerpt\") or \"\"\n",
    "                    url = item.get(\"url\") or item.get(\"link\") or \"\"\n",
    "                    summaries.append(f\"{idx}. {title}\\n{snippet}\\n{url}\")\n",
    "                return \"\\n\\n\".join(summaries) if summaries else \"No search results found.\"\n",
    "            # fallback\n",
    "            return str(results)\n",
    "        except Exception as exc:\n",
    "            return f\"Tavily search failed: {exc}\"\n",
    "\n",
    "    return tavily_search\n",
    "\n",
    "\n",
    "#\n",
    "# 3) Prepare the LLM (Gemini via langchain-google-genai)\n",
    "#\n",
    "def get_llm():\n",
    "    # Use the chat-focused wrapper\n",
    "    # model name can be adjusted: \"gemini-2.0-flash\", \"gemini-2.5-pro\", etc.\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.0)  # deterministic\n",
    "    return llm\n",
    "\n",
    "\n",
    "#\n",
    "# 4) Bring everything together: create a tool-calling agent and run it\n",
    "#\n",
    "def build_agent_and_run(destination: str):\n",
    "    llm = get_llm()\n",
    "\n",
    "    # create tavily tool\n",
    "    tavily_tool = make_tavily_search_tool()\n",
    "\n",
    "    # Tools list: LangChain expects a list of tool callables (decorated with @tool)\n",
    "    tools = [get_current_weather, tavily_tool]\n",
    "\n",
    "    # Prompt template for the agent: must contain agent_scratchpad placeholder per docs\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an intelligent travel assistant. Use the tools as needed.\"),\n",
    "            (\"placeholder\", MessagesPlaceholder(variable_name=\"chat_history\")),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", MessagesPlaceholder(variable_name=\"agent_scratchpad\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a tool-calling agent that can call our tools\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "    # AgentExecutor will execute the agent loop and call tools as necessary\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    # Run the agent with the user's query (we give it an input that asks for both weather and attractions)\n",
    "    user_prompt = f\"User is traveling to {destination}. Provide (1) current weather forecast for {destination} and (2) top attractions to visit there (concise list).\"\n",
    "\n",
    "    print(\"\\n>>> Running agent... (this may take a few seconds)\\n\")\n",
    "    result = agent_executor.invoke({\"input\": user_prompt})\n",
    "\n",
    "    # result is usually an AgentFinish or a final message object; print result contents\n",
    "    # The AgentExecutor returns an object; to be robust we attempt to pretty-print common fields:\n",
    "    try:\n",
    "        # Some LangChain versions return `.output` or `.result` or a dict-like object\n",
    "        if hasattr(result, \"output\"):\n",
    "            print(\"Agent output:\\n\", result.output)\n",
    "        elif isinstance(result, dict) and \"output\" in result:\n",
    "            print(\"Agent output:\\n\", result[\"output\"])\n",
    "        else:\n",
    "            # fallback: print the result object directly\n",
    "            print(\"Agent result (raw):\\n\", result)\n",
    "    except Exception as exc:\n",
    "        print(\"Could not format agent response:\", exc)\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77554b-07ee-44a9-86c7-e04a1d60282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run the Travel Assistant.\")\n",
    "parser.add_argument(\"destination\", type=str, help=\"Destination city (e.g., 'Chennai')\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "build_agent_and_run(args.destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5042679-adaf-455d-b18b-23ff554b1d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
