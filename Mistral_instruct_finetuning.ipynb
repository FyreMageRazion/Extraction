{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce8e244-cf80-4118-a446-4166c1964bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.42.0\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets>=2.18.0\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate>=0.30.0\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes>=0.43.0\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting peft>=0.11.1\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl>=0.9.6\n",
      "  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.42.0)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.42.0)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.42.0)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.42.0)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers>=4.42.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.18.0)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.18.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.18.0)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets>=2.18.0)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.18.0)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0) (2024.10.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.30.0) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.30.0) (2.8.0.dev20250319+cu128)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42.0) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42.0)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.30.0) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=2.0.0->accelerate>=0.30.0) (77.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.18.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.18.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.18.0)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.18.0) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.30.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.30.0) (2.1.5)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading trl-0.22.2-py3-none-any.whl (544 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m281.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m183.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m227.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m192.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, sentencepiece, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, bitsandbytes, accelerate, peft, datasets, trl\n",
      "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 bitsandbytes-0.47.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 hf-xet-1.1.9 huggingface-hub-0.34.4 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.2 peft-0.17.1 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.9.1 safetensors-0.6.2 sentencepiece-0.2.1 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.0 trl-0.22.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.42.0\" \"datasets>=2.18.0\" \"accelerate>=0.30.0\" \"bitsandbytes>=0.43.0\" \"peft>=0.11.1\" \"trl>=0.9.6\" sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0876f18-ca19-4c82-a624-9eab9fdc4434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af202f02-513c-4019-8ccf-af266f7fd771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60283f3a4ff04c0e9c10063fde4c8c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0811315-58a3-4b30-813a-e9086653487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6283cf-63f8-46d0-8e09-0489d456c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Config (edit to taste)\n",
    "# --------------------------\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "DATASET_NAME = \"yahma/alpaca-cleaned\"\n",
    "OUTPUT_DIR = \"./mistral7b-instruct-alpaca-lora\"\n",
    "SEED = 42\n",
    "\n",
    "MAX_SEQ_LEN = 1024                 # Reduce if you hit OOM; raise if you have VRAM headroom\n",
    "TRAIN_BATCH_SIZE = 1               # per-device train batch size (QLoRA, single GPU)\n",
    "GRADIENT_ACCUM_STEPS = 16\n",
    "EVAL_BATCH_SIZE = 1\n",
    "NUM_TRAIN_EPOCHS = 1.0             # Increase for better quality (2-3+)\n",
    "LEARNING_RATE = 2e-4\n",
    "LOGGING_STEPS = 20\n",
    "SAVE_STEPS = 500\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "LORA_R = 1\n",
    "LORA_ALPHA = 2\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "]  # typical for Mistral/LLaMA-class models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9314181d-eddb-4bac-aa41-0aeddc4fbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts for quick A/B comparison later\n",
    "TEST_PROMPTS = [\n",
    "    \"Write a short, friendly email to thank a colleague for helping you debug a tricky issue.\",\n",
    "    \"Explain the difference between supervised and unsupervised learning to a beginner.\",\n",
    "    \"Give me three creative uses of a paperclip.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d75ef815-4851-4d2d-9bb0-32b868fba73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Utilities / Repro\n",
    "# --------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8287cf20-42fe-44e0-ab4e-1e07f602e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Preparing 4-bit config...\n",
      "Loading base model in 4-bit (QLoRA)...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load tokenizer & base model (4-bit for QLoRA)\n",
    "# --------------------------\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "# Mistral models often have no pad token configured—align to eos to avoid warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Preparing 4-bit config...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "print(\"Loading base model in 4-bit (QLoRA)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "225d4617-2f97-4da0-9a9b-5cdb14eb0afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb0f0f5e875433b8123b39cb3cdd0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "# Enable gradient checkpointing for memory savings\n",
    "base_model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5e0643-6fd9-430b-9fff-ec025c872910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: yahma/alpaca-cleaned\n",
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe533eeceb14654bab0c8135d934162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Dataset: load & format\n",
    "# --------------------------\n",
    "print(\"Loading dataset:\", DATASET_NAME)\n",
    "ds = load_dataset(DATASET_NAME)\n",
    "\n",
    "# alpaca-cleaned has fields: instruction, input, output\n",
    "# We'll convert each row into a single string that contains a \"User:\" prompt and an \"Assistant:\" target.\n",
    "# This keeps things simple for SFT; the model learns to continue from Assistant: ...\n",
    "def format_sample(example: Dict) -> str:\n",
    "    instr = example.get(\"instruction\", \"\").strip()\n",
    "    ipt = example.get(\"input\", \"\").strip()\n",
    "    output = example.get(\"output\", \"\").strip()\n",
    "\n",
    "    if ipt:\n",
    "        user = f\"{instr}\\n\\nInput:\\n{ipt}\"\n",
    "    else:\n",
    "        user = instr\n",
    "\n",
    "    # Simple, consistent pattern:\n",
    "    # User: <...>\\nAssistant: <...>\n",
    "    # During training, we feed \"User: ...\\nAssistant: {label tokens}\" so the model learns the completion.\n",
    "    prompt = f\"User:\\n{user}\\n\\nAssistant:\\n{output}\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Formatting dataset...\")\n",
    "# train_dataset = ds[\"train\"].shuffle(seed=SEED).map(\n",
    "#     lambda x: {\"text\": format_sample(x)},\n",
    "#     remove_columns=ds[\"train\"].column_names,\n",
    "# )\n",
    "\n",
    "train_dataset = ds[\"train\"].map(\n",
    "    lambda x: {\"text\": format_sample(x)},\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Subsample dataset for quicker training (~30 min)\n",
    "MAX_SAMPLES = 2000   # try 1000 if you want even faster\n",
    "train_dataset = train_dataset.shuffle(seed=SEED).select(range(MAX_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b793ce33-40ad-4e27-a637-85c357980a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# LoRA config\n",
    "# --------------------------\n",
    "print(\"Setting up LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be1d9fec-1385-4a77-a7f7-3910890d7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training arguments...\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Training Args\n",
    "# --------------------------\n",
    "print(\"Preparing training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    fp16=not (torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    optim=\"paged_adamw_8bit\",   # paged optimizer is good with QLoRA\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9196989b-9bd6-424b-b3b8-3feca73c5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb5f37df-3453-4246-a756-4d644d911b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a2efbf6ca84244ad9da4c7b4a4a213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7972b5b2dcc46b4b0b680f93a45ffdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bee60af36c4806b661eadaf4484807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e65f052f5d47d2945a3fea0ae9b9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# SFT Trainer\n",
    "# --------------------------\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "def formatting_func(example):\n",
    "    # We already stored the full prompt-response in \"text\"\n",
    "    return example[\"text\"]\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    formatting_func=formatting_func,  \n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer, \n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfd35e68-c4ee-4924-bc9a-408ce5e1760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 15:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.944500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.9924676818847656, metrics={'train_runtime': 959.2874, 'train_samples_per_second': 2.085, 'train_steps_per_second': 0.13, 'total_flos': 1.6037281592205312e+16, 'train_loss': 0.9924676818847656, 'entropy': 1.026659072190523, 'num_tokens': 375593.0, 'mean_token_accuracy': 0.719572489708662, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Train\n",
    "# --------------------------\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51af6a81-0f98-449f-bb82-1e15eb1b6004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter to: ./mistral7b-instruct-alpaca-lora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./mistral7b-instruct-alpaca-lora/tokenizer_config.json',\n",
       " './mistral7b-instruct-alpaca-lora/special_tokens_map.json',\n",
       " './mistral7b-instruct-alpaca-lora/chat_template.jinja',\n",
       " './mistral7b-instruct-alpaca-lora/tokenizer.model',\n",
       " './mistral7b-instruct-alpaca-lora/added_tokens.json',\n",
       " './mistral7b-instruct-alpaca-lora/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Save LoRA adapter\n",
    "# --------------------------\n",
    "print(\"Saving LoRA adapter to:\", OUTPUT_DIR)\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90999da2-0809-4fd0-a7a0-285cdf012e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Inference comparison: Base vs Fine-tuned (LoRA)\n",
    "# --------------------------\n",
    "def build_chat_prompt(user_text: str) -> str:\n",
    "    # Use the same simple pattern we trained with for consistency:\n",
    "    return f\"User:\\n{user_text}\\n\\nAssistant:\\n\"\n",
    "\n",
    "def generate_with_model(model, tok, prompt: str, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    # Return only the assistant continuation after our prompt marker:\n",
    "    if \"Assistant:\" in text:\n",
    "        return text.split(\"Assistant:\", 1)[1].strip()\n",
    "    return text\n",
    "\n",
    "def load_base_pipeline():\n",
    "    # Fresh base model for a clean comparison (no adapters)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_finetuned_pipeline():\n",
    "    # Load base, then attach LoRA adapter we just trained\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a98541be-c3ad-4cda-a4f6-a9604fb41d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference: Comparing Base vs Fine-tuned ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04961a752304667accab32739fb070a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bde4b7b515c404cbb70acd71d0657bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt 1 ===\n",
      ">> User: Write a short, friendly email to thank a colleague for helping you debug a tricky issue.\n",
      "\n",
      "[BASE MODEL RESPONSE]\n",
      "Subject: Heartfelt thanks for your assistance with the issue\n",
      "\n",
      "Dear [Colleague's Name],\n",
      "\n",
      "I hope this email finds you well. I wanted to take a moment to express my gratitude for the help you provided me with the debugging issue we encountered recently. Your patience, expertise, and willingness to work together made all the difference.\n",
      "\n",
      "I must admit, I was quite stumped by the problem, and I couldn't have solved it without your assistance. The issue has been resolved, and I am truly grateful for your support.\n",
      "\n",
      "In the future, if I can ever be of help to you, please don't hesitate to ask. I am always here to lend a hand.\n",
      "\n",
      "Once again, thank you for your help. I look forward to working with you on our next project.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "[FINE-TUNED (LoRA) RESPONSE]\n",
      "Subject: Thank you for your help debugging the issue\n",
      "\n",
      "Dear [Colleague's name],\n",
      "\n",
      "I wanted to take a moment to thank you for your assistance in helping me debug the issue we were experiencing yesterday. Your expertise and guidance were invaluable, and I truly appreciate the time and effort you put into helping me find a solution.\n",
      "\n",
      "Your patience and kindness in explaining the issue and the steps to resolve it were greatly appreciated. I am grateful to have such a knowledgeable and supportive colleague to work with.\n",
      "\n",
      "If there is anything I can do to help you in the future, please do not hesitate to ask. Thank you again for your help and support.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your name]\n",
      "\n",
      "=== Prompt 2 ===\n",
      ">> User: Explain the difference between supervised and unsupervised learning to a beginner.\n",
      "\n",
      "[BASE MODEL RESPONSE]\n",
      "In machine learning, we can broadly categorize learning methods into two main types: supervised and unsupervised learning.\n",
      "\n",
      "1. Supervised Learning: This is like learning from a teacher who provides examples and answers. The teacher, in this case, is an algorithm that uses labeled data. Labeled data is data that has already been categorized or classified. The goal of supervised learning is to predict the output or class of new, unseen data. Examples include classification (e.g., email spam detection) and regression (e.g., predicting house prices).\n",
      "\n",
      "2. Unsupervised Learning: Unlike supervised learning, there are no labeled examples provided. Instead, the algorithm tries to find hidden patterns or structures in the data on its own. The goal is often to group similar data points together (clustering), or to find a representation that captures the essential information in the data (dimensionality reduction). An example of unsupervised learning is image segmentation (grouping similar pixels together in an image).\n",
      "\n",
      "In summary, supervised learning learns from labeled examples, while unsupervised learning finds patterns in unlabeled data. Both are important tools in the machine learning toolkit.\n",
      "\n",
      "[FINE-TUNED (LoRA) RESPONSE]\n",
      "Supervised and unsupervised learning are two main types of machine learning, which are methods used by computer systems to learn from data and make decisions or predictions.\n",
      "\n",
      "Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data is accompanied by the correct output. The algorithm is trained to learn the relationship between the input and output, and once it has been trained, it can make predictions on new, unseen data. This type of learning is often used in tasks such as image classification, speech recognition, and spam filtering.\n",
      "\n",
      "On the other hand, unsupervised learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that the input data is not accompanied by the correct output. Instead, the algorithm is tasked with finding patterns or structure in the data on its own. This type of learning is often used in tasks such as clustering, anomaly detection, and dimensionality reduction.\n",
      "\n",
      "In summary, supervised learning involves training an algorithm on labeled data to make predictions on new, unseen data, while unsupervised learning involves finding patterns or structure in unlabeled data. Both types of learning are important in the field of\n",
      "\n",
      "=== Prompt 3 ===\n",
      ">> User: Give me three creative uses of a paperclip.\n",
      "\n",
      "[BASE MODEL RESPONSE]\n",
      "1. Paperclip Bookmark: Use a paperclip as a bookmark by straightening it out and slipping it between the pages of a book.\n",
      "2. DIY Earring Back: If you've lost an earring back, you can use a paperclip as a temporary solution. Straighten one end and bend it back into a small loop.\n",
      "3. Desk Organizer: Bend a paperclip into various shapes to create small desk organizers for items like paperclips, bobby pins, or safety pins.\n",
      "\n",
      "[FINE-TUNED (LoRA) RESPONSE]\n",
      "1. Organize your desk: Use paperclips to group similar items together, such as pens, pencils, and paper clips. This will help you keep your workspace organized and tidy.\n",
      "\n",
      "2. Bookmark: Paperclips can be used as a bookmark to mark your place in a book. Simply clip the paperclip to the corner of the page you want to remember.\n",
      "\n",
      "3. Jewelry: Use paperclips to create unique and creative jewelry pieces. Bend the paperclip into a shape or form, and attach it to a chain or cord to wear as a necklace or bracelet. You can also use multiple paperclips to create more intricate designs.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Inference: Comparing Base vs Fine-tuned ---\")\n",
    "base_infer_model = load_base_pipeline()\n",
    "ft_infer_model = load_finetuned_pipeline()\n",
    "\n",
    "for i, uprompt in enumerate(TEST_PROMPTS, start=1):\n",
    "    print(f\"\\n=== Prompt {i} ===\")\n",
    "    print(\">> User:\", uprompt)\n",
    "\n",
    "    prompt = build_chat_prompt(uprompt)\n",
    "\n",
    "    base_resp = generate_with_model(base_infer_model, tokenizer, prompt)\n",
    "    print(\"\\n[BASE MODEL RESPONSE]\")\n",
    "    print(base_resp)\n",
    "\n",
    "    ft_resp = generate_with_model(ft_infer_model, tokenizer, prompt)\n",
    "    print(\"\\n[FINE-TUNED (LoRA) RESPONSE]\")\n",
    "    print(ft_resp)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea01824-00c0-47c7-aa36-9c61380fe4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41ad4f-9fe1-4063-bb18-1936fdf92720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbf755-4e37-4806-a10b-9010740dba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dc5c2-e050-4eea-b6a6-765b5f8c92da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
