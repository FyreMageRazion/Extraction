{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51817fc-d5bb-4bbc-8ce6-d1bfed13e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai pydantic\n",
    "# !pip install PyPDF2 google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d636b761-727e-461e-b083-23a8e961fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab60680-6925-4bed-a2ab-27e5d01ded3c",
   "metadata": {},
   "source": [
    "## Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9937cda4-6d72-4e30-b52b-bc9bc07bece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = \"<API_KEY>\"\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf09dc-289c-4af2-a372-c84dcfc75efd",
   "metadata": {},
   "source": [
    "## Configure PDF Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f517ba0a-2110-4701-b3f7-41544437005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"Prompt Engineering.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70d974bd-7fd4-4aec-a61a-827cd99f82f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4917 characters from the PDF.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "print(f\"Extracted {len(document_text)} characters from the PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9c7ad1a6-71fe-45f2-a3d2-e9afb51d8cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre-processing as observed many '\\n' inbetween\n",
    "document_text = document_text.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4934e-df34-4cee-9a81-f86f493c958f",
   "metadata": {},
   "source": [
    "## Getting Topic Hint\n",
    "One time inference with top 500 characters to get overall topic so that the prompt that comes later can be adapted to this identified topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17be1832-0193-4386-a37e-fd28586d3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_topic_hint(text: str, max_chars: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Use Gemini-2.0-flash to infer a short, friendly study topic hint from the first part of the text.\n",
    "    This is done once, instead of using multiple heuristics.\n",
    "    \"\"\"\n",
    "    snippet = text.strip().replace(\"\\n\", \" \")[:max_chars]\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are an intelligent study assistant. Based on the following text snippet, \"\n",
    "        f\"provide a short, clear, friendly topic title suitable for study purposes. \"\n",
    "        f\"Keep it concise (max 6-8 words) and descriptive.\\n\\nText snippet:\\n{snippet}\\n\\n\"\n",
    "        f\"Output only the topic title:\"\n",
    "    )\n",
    "\n",
    "    # Gemini API call\n",
    "    response = model.generate_content(prompt)\n",
    "    # Depending on the API, response might be a dict or object; extract text\n",
    "    topic = response.text.strip() if hasattr(response, \"text\") else str(response).strip()\n",
    "\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9f63105-bf6a-4bc1-86ff-1bad690486f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering: Guiding AI with Text\n"
     ]
    }
   ],
   "source": [
    "topic_hint = detect_topic_hint(document_text,500)\n",
    "print(topic_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92646ae-7222-4bcf-8d57-6f4d3604f272",
   "metadata": {},
   "source": [
    "## Summary & MCQ prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "069f9db5-9f9f-4de5-ab2e-212b243b5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = PromptTemplate(\n",
    "input_variables=[\"chunk_text\", \"topic_hint\"],\n",
    "    template=(\n",
    "        \"You are an expert study assistant.\\n\"\n",
    "        \"Summarize the following content into concise bullet points suitable for quick review.\\n\"\n",
    "        \"Be brief, use 6-12 bullets if content is long, and include any important formulas, dates, or definitions.\\n\"\n",
    "        \"Adapt your tone to be friendly and clear for someone studying {topic_hint}.\\n\\n\"\n",
    "        \"Content:\\n{chunk_text}\\n\\n\"\n",
    "        \"Summary (bullet points):\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "MCQ_PROMPT = PromptTemplate(\n",
    "input_variables=[\"summary_text\", \"num_questions\", \"topic_hint\"],\n",
    "    template=(\n",
    "        \"You are a friendly teacher preparing a short multiple-choice quiz from the study notes below.\\n\"\n",
    "        \"Generate exactly {num_questions} multiple-choice questions that test important concepts from the summary.\\n\"\n",
    "        \"For each question: provide a clear question statement, 4 answer options labeled A-D, and mark the correct answer.\\n\"\n",
    "        \"At the end of each question, include a one-sentence explanation for why the correct answer is correct.\\n\"\n",
    "        \"Make sure distractors (wrong options) are plausible. Include the topic: {topic_hint}.\\n\\n\"\n",
    "        \"Study notes:\\n{summary_text}\\n\\n\"\n",
    "        \"Output format (use this strictly):\\n\"\n",
    "        \"Q<n>. <question text>\\n\"\n",
    "        \"A) option 1\\n\"\n",
    "        \"B) option 2\\n\"\n",
    "        \"C) option 3\\n\"\n",
    "        \"D) option 4\\n\"\n",
    "        \"Answer: <A/B/C/D>\\n\"\n",
    "        \"Explanation: <one-sentence explanation>\\n\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f0982-ee78-48ff-90dc-7550295ae027",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49513515-8417-4187-b07e-97562fca46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 2000, chunk_overlap: int = 200) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "    \n",
    "def summarize_chunks(llm, chunks: List[str], topic_hint: str) -> List[str]:\n",
    "    chain = LLMChain(llm=llm, prompt=SUMMARY_PROMPT)\n",
    "    summaries = []\n",
    "    for idx, c in enumerate(chunks, start=1):\n",
    "        out = chain.run({\"chunk_text\": c, \"topic_hint\": topic_hint})\n",
    "        summaries.append(out.strip())\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218060-c007-47f1-858d-6126a9583c3a",
   "metadata": {},
   "source": [
    "## Gemini Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba4f7-648a-417c-90f7-25df27789102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_summaries(llm, partial_summaries: List[str], topic_hint: str) -> str:\n",
    "    combined = \"\\n\\n\".join(partial_summaries)\n",
    "    final_prompt = PromptTemplate(\n",
    "    input_variables=[\"combined_text\", \"topic_hint\"],\n",
    "    template=(\n",
    "        \"You are an expert study assistant. Condense the following partial summaries into a single, concise set of 8-12 bullet points for quick studying.\\n\"\n",
    "        \"Preserve important facts, formulas, and definitions. Adapt the wording for clarity for {topic_hint}.\\n\\n\"\n",
    "        \"{combined_text}\\n\\n\"\n",
    "        \"Final summary (bullets):\"\n",
    "        )\n",
    "        )\n",
    "    final_chain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "    final = final_chain.run({\"combined_text\": combined, \"topic_hint\": topic_hint})\n",
    "    return final.strip()\n",
    "\n",
    "def generate_mcqs(llm, final_summary: str, topic_hint: str, num_questions: int = 6) -> str:\n",
    "    mcq_chain = LLMChain(llm=llm, prompt=MCQ_PROMPT)\n",
    "    mcq_output = mcq_chain.run({\"summary_text\": final_summary, \"num_questions\": str(num_questions), \"topic_hint\": topic_hint})\n",
    "    return mcq_output.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662d4f3-d7c4-4fb2-9f82-ddd6997150d3",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6c83f-0dec-42bd-8692-a1a7141ded6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StudyOutput:\n",
    "    summary: str\n",
    "    mcq_text: str\n",
    "\n",
    "def process_pdf_text(pdf_text: str, llm, topic_hint, num_questions: int = 6) -> StudyOutput:\n",
    "    chunks = chunk_text(pdf_text)\n",
    "    print(f\"Debug: created {len(chunks)} chunks. Topic hint: {topic_hint}\")\n",
    "\n",
    "    partial_summaries = summarize_chunks(llm, chunks, topic_hint)\n",
    "    final_summary = combine_summaries(llm, partial_summaries, topic_hint)\n",
    "    mcq_text = generate_mcqs(llm, final_summary, topic_hint, num_questions=num_questions)\n",
    "    \n",
    "    return StudyOutput(summary=final_summary, mcq_text=mcq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f1850-7739-4c5b-b13a-90eb2b35d2fe",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2f0c3d0e-1b71-4523-9475-f38de4f82380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: created 3 chunks. Topic hint: Prompt Engineering: Guiding AI with Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_19496\\3762355438.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = chain.run({\"chunk_text\": c, \"topic_hint\": topic_hint})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL SUMMARY =====\n",
      "Here's a concise summary of Prompt Engineering for quick studying:\n",
      "\n",
      "*   **Prompt Engineering:** Guiding AI models with text-based instructions (prompts) to achieve specific tasks.\n",
      "*   **Prompt Components:** Effective prompts include clear instructions, relevant context, input data (if needed), and a desired output indicator.\n",
      "*   **Prompt Types:** Text prompts (text generation), Code prompts (code tasks), Image prompts (image generation).\n",
      "*   **Zero-Shot Prompting:** Model performs task based on general knowledge, without specific examples.\n",
      "*   **Few-Shot Prompting:** Provide a few examples in the prompt to guide the model's output.\n",
      "*   **Chain-of-Thought (CoT):** Break down complex tasks into intermediate reasoning steps for improved reasoning.\n",
      "*   **Prompt Improvement:** Use role-playing (assign personas), be concise and specific, and maintain a consistent tone.\n",
      "*   **Avoid Overload & Vagueness:** Keep prompts focused and avoid open-ended questions for precise results.\n",
      "*   **Constraints:** Use constraints wisely to guide the model by setting boundaries and requirements.\n",
      "*   **User-Model Interaction:** Follow a clear pattern: User provides instruction, model provides response. (User: <Instruction> Model: <Response>)\n",
      "\n",
      "===== MCQs =====\n",
      "Q1. Which of the following is NOT typically considered a core component of an effective prompt in prompt engineering?\n",
      "A) Clear instructions\n",
      "B) Relevant context\n",
      "C) Randomly generated text\n",
      "D) Desired output indicator\n",
      "Answer: C\n",
      "Explanation: Effective prompts require clear guidance, context, and a desired output, not random text.\n",
      "\n",
      "Q2. Which prompting technique involves providing a few examples within the prompt to guide the model's response?\n",
      "A) Zero-Shot Prompting\n",
      "B) Few-Shot Prompting\n",
      "C) Chain-of-Thought Prompting\n",
      "D) Constraint Prompting\n",
      "Answer: B\n",
      "Explanation: Few-shot prompting uses examples to demonstrate the desired output format and content.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "out = process_pdf_text(document_text, llm, topic_hint, num_questions=2)\n",
    "\n",
    "print('\\n===== FINAL SUMMARY =====')\n",
    "print(out.summary)\n",
    "print('\\n===== MCQs =====')\n",
    "print(out.mcq_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
